{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kksahoo02/Code-Energy-Prediction/blob/main/STATIC_FEATURES.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PURE STATIC FEATURES - Final Debugged Script\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.ensemble import IsolationForest, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from lightgbm import LGBMRegressor, log_evaluation\n",
        "\n",
        "# ==========================\n",
        "# CONFIG\n",
        "# ==========================\n",
        "DATASET_PATH = '/content/ml_code_dataset.csv'  # <- update to your file\n",
        "TARGET = 'energy_consumption_joules'\n",
        "\n",
        "# Lists (static/dynamic/metadata)\n",
        "DYNAMIC_FEATURES = [\n",
        "    'execution_time', 'runtime_seconds', 'elapsed_time', 'duration',\n",
        "    'memory_peak_mb', 'memory_avg_mb', 'memory_usage', 'memory_allocated',\n",
        "    'memory_reserved', 'memory_max', 'ram_usage', 'heap_size',\n",
        "    'cpu_percent_avg', 'cpu_percent_peak', 'cpu_usage', 'cpu_time',\n",
        "    'user_time', 'system_time', 'cpu_cycles',\n",
        "    'disk_read_mb', 'disk_write_mb', 'io_read', 'io_write',\n",
        "    'disk_operations', 'read_bytes', 'write_bytes',\n",
        "    'co2_emissions_g', 'power_draw_watts', 'energy_pkg_joules',\n",
        "    'energy_ram_joules', 'energy_consumption', 'carbon_footprint',\n",
        "    'gpu_used', 'gpu_memory', 'gpu_utilization', 'cuda_memory',\n",
        "    'network_sent_mb', 'network_recv_mb', 'bytes_sent', 'bytes_received',\n",
        "    'packets_sent', 'packets_received',\n",
        "    'process_count', 'thread_count', 'context_switches', 'num_threads',\n",
        "    'num_processes', 'child_processes',\n",
        "    'cache_misses', 'cache_hits', 'instructions_retired', 'cycles_elapsed',\n",
        "    'branch_misses', 'page_faults', 'tlb_misses',\n",
        "    'runtime_errors', 'memory_errors', 'exception_count', 'error_count',\n",
        "    'warning_count', 'num_exceptions',\n",
        "    'output_size', 'return_value', 'stdout_length', 'stderr_length',\n",
        "    'output_lines', 'print_statements_executed',\n",
        "    'system_load', 'available_memory', 'free_disk', 'temperature',\n",
        "]\n",
        "\n",
        "METADATA_COLS = [\n",
        "    'file_path', 'file_name', 'repo_name', 'repo_url', 'timestamp',\n",
        "    'executed_successfully', 'syntax_valid', 'run_id', 'index', 'id',\n",
        "    'file_id', 'repository', 'author', 'date_created', 'date_modified'\n",
        "]\n",
        "\n",
        "dynamic_keywords = [\n",
        "    'time', 'memory', 'cpu', 'disk', 'io', 'network', 'gpu',\n",
        "    'peak', 'avg', 'max', 'min', 'usage', 'consumed', 'allocated',\n",
        "    'runtime', 'execution', 'process', 'thread', 'energy', 'power',\n",
        "    'emission', 'carbon', 'watt', 'joule', 'exception', 'error',\n",
        "    'output', 'result', 'performance', 'utilization', 'load'\n",
        "]\n",
        "\n",
        "# Helper for safe column printing\n",
        "def safe_print_cols(cols, title=None, max_show=40):\n",
        "    if title:\n",
        "        print(title)\n",
        "    for c in (cols[:max_show]):\n",
        "        print(f\"   - {c}\")\n",
        "    if len(cols) > max_show:\n",
        "        print(f\"   ... and {len(cols)-max_show} more\")\n",
        "\n",
        "# ==========================\n",
        "# LOAD & BASIC FILTERING\n",
        "# ==========================\n",
        "print(\"=\" * 80)\n",
        "print(\"PURE STATIC FEATURES MODEL - DEBUGGED\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "print(f\"\\nðŸ“Š Initial dataset: {len(df):,} rows Ã— {df.shape[1]} columns\")\n",
        "\n",
        "# Filter for executed_successfully if available\n",
        "if 'executed_successfully' in df.columns:\n",
        "    before = len(df)\n",
        "    df = df[df['executed_successfully'] == True].copy()\n",
        "    after = len(df)\n",
        "    print(f\"âœ… After filtering executed_successfully: {after:,} rows ({after/before*100:.1f}% of pre-filter)\")\n",
        "\n",
        "# Ensure target exists and remove zero/NaN\n",
        "if TARGET not in df.columns:\n",
        "    raise SystemExit(f\"ERROR: Target '{TARGET}' not found. Columns: {df.columns.tolist()}\")\n",
        "\n",
        "df = df.dropna(subset=[TARGET])\n",
        "df = df[df[TARGET] > 0].copy()\n",
        "print(f\"âœ… After removing zero/NaN target: {len(df):,} rows\")\n",
        "\n",
        "if len(df) < 10:\n",
        "    raise SystemExit(\"ERROR: Too few rows remain after initial filtering to train a model safely.\")\n",
        "\n",
        "# ==========================\n",
        "# IDENTIFY PURE STATIC FEATURES\n",
        "# ==========================\n",
        "print(\"\\nðŸ” Identifying PURE STATIC features...\")\n",
        "\n",
        "all_cols = df.columns.tolist()\n",
        "candidates = [c for c in all_cols if c not in (DYNAMIC_FEATURES + METADATA_COLS + [TARGET])]\n",
        "\n",
        "static_features = []\n",
        "removed_suspicious = []\n",
        "for col in candidates:\n",
        "    low = col.lower()\n",
        "    if any(k in low for k in dynamic_keywords):\n",
        "        removed_suspicious.append(col)\n",
        "    else:\n",
        "        static_features.append(col)\n",
        "\n",
        "print(f\"\\nðŸ“‹ Feature filtering summary:\")\n",
        "print(f\"   Total columns: {len(all_cols)}\")\n",
        "print(f\"   Removed metadata columns: {len([c for c in all_cols if c in METADATA_COLS])}\")\n",
        "print(f\"   Removed explicit dynamic features: {len([c for c in all_cols if c in DYNAMIC_FEATURES])}\")\n",
        "print(f\"   Removed suspicious keyword matches: {len(removed_suspicious)}\")\n",
        "print(f\"   âœ… Candidate static features (pre-numeric): {len(static_features)}\")\n",
        "\n",
        "if removed_suspicious:\n",
        "    safe_print_cols(removed_suspicious, title=\"âš ï¸ Example removed suspicious columns:\", max_show=20)\n",
        "\n",
        "if len(static_features) == 0:\n",
        "    print(\"\\nâŒ No static features detected. Listing all columns for debugging:\")\n",
        "    safe_print_cols(all_cols, title=\"All columns:\")\n",
        "    raise SystemExit(\"No static features found. Adjust lists or dataset.\")\n",
        "\n",
        "# ==========================\n",
        "# KEEP ONLY NUMERIC STATIC FEATURES\n",
        "# ==========================\n",
        "X_raw = df[static_features].copy()\n",
        "y = df[TARGET].copy()\n",
        "\n",
        "# Coerce non-numeric to numeric where possible (non-convertible => NaN)\n",
        "for col in X_raw.columns:\n",
        "    if not pd.api.types.is_numeric_dtype(X_raw[col]):\n",
        "        X_raw[col] = pd.to_numeric(X_raw[col], errors='coerce')\n",
        "\n",
        "# Drop columns that became all-NaN\n",
        "num_cols = [c for c in X_raw.columns if X_raw[c].notna().any()]\n",
        "X = X_raw[num_cols].copy()\n",
        "dropped_non_numeric = set(static_features) - set(num_cols)\n",
        "if dropped_non_numeric:\n",
        "    print(f\"\\nâ„¹ï¸ Dropped non-numeric/all-NaN static features: {len(dropped_non_numeric)}\")\n",
        "    safe_print_cols(sorted(list(dropped_non_numeric)), title=\" Examples dropped:\", max_show=20)\n",
        "\n",
        "print(f\"\\nðŸ“ Feature matrix: {X.shape[0]:,} rows Ã— {X.shape[1]} features\")\n",
        "\n",
        "if X.shape[1] == 0:\n",
        "    raise SystemExit(\"ERROR: No numeric static features available after coercion.\")\n",
        "\n",
        "# ==========================\n",
        "# CLEAN INF / MISSING\n",
        "# ==========================\n",
        "print(\"\\nðŸ§¹ Cleaning feature matrix (infs -> NaN, drop high-missing, fill medians)...\")\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "missing_pct = X.isna().sum() / len(X)\n",
        "cols_to_drop = missing_pct[missing_pct > 0.5].index.tolist()\n",
        "if cols_to_drop:\n",
        "    X.drop(columns=cols_to_drop, inplace=True)\n",
        "    print(f\"   Dropped {len(cols_to_drop)} columns with >50% missing\")\n",
        "\n",
        "# Fill remaining NaNs with median\n",
        "for col in X.columns:\n",
        "    if X[col].isna().sum() > 0:\n",
        "        X[col].fillna(X[col].median(), inplace=True)\n",
        "\n",
        "print(f\"âœ… After cleaning: {X.shape[1]} features remain\")\n",
        "\n",
        "# ==========================\n",
        "# REMOVE CONSTANT / LOW VARIANCE\n",
        "# ==========================\n",
        "print(\"\\nðŸ”§ Removing constant and very low-variance features...\")\n",
        "constant_cols = [col for col in X.columns if X[col].nunique() <= 1]\n",
        "if constant_cols:\n",
        "    X.drop(columns=constant_cols, inplace=True)\n",
        "    print(f\"   Removed {len(constant_cols)} constant features\")\n",
        "\n",
        "if X.shape[1] > 0:\n",
        "    vt = VarianceThreshold(threshold=0.0)\n",
        "    vt.fit(X)\n",
        "    variances = vt.variances_\n",
        "    variance_cutoff = np.percentile(variances, 5)\n",
        "    keep_mask = variances > variance_cutoff\n",
        "    cols_kept = X.columns[keep_mask]\n",
        "    X = X[cols_kept]\n",
        "    print(f\"   After low-variance filtering: {X.shape[1]} features remain\")\n",
        "\n",
        "if X.shape[1] == 0:\n",
        "    raise SystemExit(\"ERROR: No features remain after variance filtering.\")\n",
        "\n",
        "print(f\"âœ… Final static feature count: {X.shape[1]}\")\n",
        "\n",
        "# ==========================\n",
        "# AGGRESSIVE OUTLIER REMOVAL\n",
        "# ==========================\n",
        "print(\"\\nðŸ” Aggressive outlier removal (IsolationForest -> quantiles -> z-score)...\")\n",
        "X_clean = X.copy()\n",
        "y_clean = y.loc[X_clean.index].copy()\n",
        "\n",
        "print(f\"   Initial samples: {X_clean.shape[0]:,}\")\n",
        "\n",
        "# IsolationForest (skip if too few samples)\n",
        "if X_clean.shape[0] > 20:\n",
        "    try:\n",
        "        iso = IsolationForest(contamination=0.10, random_state=42, n_estimators=200, max_samples='auto')\n",
        "        pred = iso.fit_predict(X_clean)\n",
        "        mask_iso = pred == 1\n",
        "        X_clean = X_clean.loc[mask_iso].copy()\n",
        "        y_clean = y_clean.loc[mask_iso].copy()\n",
        "        print(f\"   After IsolationForest (10% removal): {X_clean.shape[0]:,} samples\")\n",
        "    except Exception as e:\n",
        "        print(f\"   âš ï¸ IsolationForest failed: {e}. Skipping this step.\")\n",
        "else:\n",
        "    print(\"   âš ï¸ Too few samples for IsolationForest - skipping.\")\n",
        "\n",
        "# Quantile filter on target\n",
        "q_low = y_clean.quantile(0.01)\n",
        "q_high = y_clean.quantile(0.97)\n",
        "mask_q = (y_clean >= q_low) & (y_clean <= q_high)\n",
        "X_clean = X_clean.loc[mask_q].copy()\n",
        "y_clean = y_clean.loc[mask_q].copy()\n",
        "print(f\"   After quantile filtering (1%-97%): {X_clean.shape[0]:,} samples\")\n",
        "\n",
        "# Z-score filter per feature\n",
        "stds = X_clean.std()\n",
        "zero_std_cols = stds[stds == 0].index.tolist()\n",
        "if zero_std_cols:\n",
        "    X_clean.drop(columns=zero_std_cols, inplace=True)\n",
        "    print(f\"   Dropped {len(zero_std_cols)} zero-std columns before z-score filtering\")\n",
        "\n",
        "if X_clean.shape[0] > 0 and X_clean.shape[1] > 0:\n",
        "    z_scores = np.abs((X_clean - X_clean.mean()) / X_clean.std(ddof=0))\n",
        "    mask_z = (z_scores < 3.5).all(axis=1)\n",
        "    X_clean = X_clean.loc[mask_z].copy()\n",
        "    y_clean = y_clean.loc[mask_z].copy()\n",
        "    print(f\"   After z-score filtering (3.5Ïƒ): {X_clean.shape[0]:,} samples\")\n",
        "else:\n",
        "    print(\"   âš ï¸ No samples/features left to perform z-score filtering.\")\n",
        "\n",
        "removed_count = X.shape[0] - X_clean.shape[0]\n",
        "print(f\"âœ… Total outliers removed (feature-rows): {removed_count:,} ({(removed_count / max(1, X.shape[0]))*100:.1f}%)\")\n",
        "\n",
        "if X_clean.shape[0] < 20:\n",
        "    print(\"âš ï¸ Warning: Very few samples remain after outlier removal. Model may be unstable.\")\n",
        "\n",
        "# ==========================\n",
        "# TARGET TRANSFORM\n",
        "# ==========================\n",
        "y_log = np.log1p(y_clean)\n",
        "print(\"\\nðŸ“ˆ Applied log1p to target:\")\n",
        "print(f\"   Original target range: [{y_clean.min():.2f}, {y_clean.max():.2f}]\")\n",
        "print(f\"   Log-transformed range: [{y_log.min():.2f}, {y_log.max():.2f}]\")\n",
        "\n",
        "# ==========================\n",
        "# SCALING\n",
        "# ==========================\n",
        "scaler = RobustScaler()\n",
        "X_scaled = pd.DataFrame(scaler.fit_transform(X_clean), columns=X_clean.columns, index=X_clean.index)\n",
        "print(\"âœ… RobustScaler applied\")\n",
        "\n",
        "# ==========================\n",
        "# TRAIN-TEST SPLIT\n",
        "# ==========================\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_log, test_size=0.2, random_state=42)\n",
        "print(f\"\\nðŸ“Š Train/Test split: train={X_train.shape[0]:,}, test={X_test.shape[0]:,}\")\n",
        "\n",
        "# ==========================\n",
        "# LIGHTGBM TRAINING (no verbose arg)\n",
        "# ==========================\n",
        "print(f\"\\nðŸš€ Training LightGBM (features: {X_train.shape[1]})\")\n",
        "lgbm = LGBMRegressor(\n",
        "    objective='regression',\n",
        "    n_estimators=2000,\n",
        "    learning_rate=0.01,\n",
        "    num_leaves=31,\n",
        "    max_depth=8,\n",
        "    min_child_samples=20,\n",
        "    subsample=0.8,\n",
        "    colsample_bytree=0.8,\n",
        "    reg_alpha=0.1,\n",
        "    reg_lambda=0.1,\n",
        "    random_state=42,\n",
        "    n_jobs=-1,\n",
        "    verbose=-1\n",
        ")\n",
        "\n",
        "# Use callback for periodic logging (optional). Remove callbacks parameter for silent run.\n",
        "lgbm.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    eval_set=[(X_test, y_test)],\n",
        "    eval_metric='rmse',\n",
        "    callbacks=[log_evaluation(100)]\n",
        ")\n",
        "\n",
        "# ==========================\n",
        "# LGBM EVALUATION\n",
        "# ==========================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"LIGHTGBM PERFORMANCE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "y_train_pred_log = lgbm.predict(X_train)\n",
        "y_test_pred_log = lgbm.predict(X_test)\n",
        "\n",
        "r2_train_log = r2_score(y_train, y_train_pred_log)\n",
        "r2_test_log = r2_score(y_test, y_test_pred_log)\n",
        "print(\"\\nðŸ“Š Log-scale RÂ²:\")\n",
        "print(f\"   Train RÂ²: {r2_train_log:.6f}\")\n",
        "print(f\"   Test  RÂ²: {r2_test_log:.6f}\")\n",
        "\n",
        "# Back to original scale\n",
        "y_train_actual = np.expm1(y_train)\n",
        "y_train_pred = np.expm1(y_train_pred_log)\n",
        "y_test_actual = np.expm1(y_test)\n",
        "y_test_pred = np.expm1(y_test_pred_log)\n",
        "\n",
        "r2_train_orig = r2_score(y_train_actual, y_train_pred)\n",
        "r2_test_orig = r2_score(y_test_actual, y_test_pred)\n",
        "mae_train = mean_absolute_error(y_train_actual, y_train_pred)\n",
        "mae_test = mean_absolute_error(y_test_actual, y_test_pred)\n",
        "rmse_train = np.sqrt(mean_squared_error(y_train_actual, y_train_pred))\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test_actual, y_test_pred))\n",
        "\n",
        "print(\"\\nâš¡ Original scale performance:\")\n",
        "print(f\"   Train RÂ²: {r2_train_orig:.6f}\")\n",
        "print(f\"   Test  RÂ²: {r2_test_orig:.6f}\")\n",
        "\n",
        "print(\"\\nðŸ“ Error metrics (Joules):\")\n",
        "print(f\"   Train MAE: {mae_train:,.2f} J\")\n",
        "print(f\"   Test  MAE: {mae_test:,.2f} J\")\n",
        "print(f\"   Train RMSE: {rmse_train:,.2f} J\")\n",
        "print(f\"   Test  RMSE: {rmse_test:,.2f} J\")\n",
        "\n",
        "mean_energy = y_test_actual.mean() if len(y_test_actual) else np.nan\n",
        "if not np.isnan(mean_energy) and mean_energy != 0:\n",
        "    print(f\"   Mean energy (test): {mean_energy:,.2f} J\")\n",
        "    print(f\"   MAE as % of mean: {(mae_test/mean_energy)*100:.2f}%\")\n",
        "\n",
        "# Feature importance\n",
        "fi = pd.DataFrame({'feature': X_train.columns, 'importance': lgbm.feature_importances_}).sort_values('importance', ascending=False)\n",
        "print(\"\\nðŸŽ¯ Top LightGBM features:\")\n",
        "for idx, row in fi.head(20).iterrows():\n",
        "    print(f\"   {row['feature']:40s} {row['importance']:8.1f}\")\n",
        "\n",
        "# ==========================\n",
        "# LGBM CROSS-VALIDATION\n",
        "# ==========================\n",
        "print(\"\\nðŸ”„ 5-Fold CV (LGBM, log-scale):\")\n",
        "try:\n",
        "    cv_scores = cross_val_score(lgbm, X_scaled, y_log, cv=5, scoring='r2', n_jobs=-1)\n",
        "    print(f\"   CV RÂ²: {[f'{s:.4f}' for s in cv_scores]}\")\n",
        "    print(f\"   Mean CV RÂ²: {cv_scores.mean():.6f} Â±{cv_scores.std():.6f}\")\n",
        "except Exception as e:\n",
        "    print(f\"   âš ï¸ CV failed: {e}\")\n",
        "\n",
        "# ==========================\n",
        "# RANDOM FOREST (max_features fixed)\n",
        "# ==========================\n",
        "print(\"\\nðŸŒ² Training RandomForestRegressor (max_features='sqrt')\")\n",
        "\n",
        "rf = RandomForestRegressor(\n",
        "    n_estimators=600,\n",
        "    max_depth=None,\n",
        "    min_samples_split=2,\n",
        "    min_samples_leaf=1,\n",
        "    max_features='sqrt',  # fixed from 'auto'\n",
        "    bootstrap=True,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "rf.fit(X_train, y_train)\n",
        "\n",
        "# RF evaluation\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"RANDOM FOREST PERFORMANCE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "rf_train_log_pred = rf.predict(X_train)\n",
        "rf_test_log_pred = rf.predict(X_test)\n",
        "\n",
        "rf_r2_train_log = r2_score(y_train, rf_train_log_pred)\n",
        "rf_r2_test_log = r2_score(y_test, rf_test_log_pred)\n",
        "print(\"\\nðŸ“Š Log-scale RÂ² (RF):\")\n",
        "print(f\"   Train RÂ²: {rf_r2_train_log:.6f}\")\n",
        "print(f\"   Test  RÂ²: {rf_r2_test_log:.6f}\")\n",
        "\n",
        "rf_train_actual = np.expm1(y_train)\n",
        "rf_train_pred = np.expm1(rf_train_log_pred)\n",
        "rf_test_actual = np.expm1(y_test)\n",
        "rf_test_pred = np.expm1(rf_test_log_pred)\n",
        "\n",
        "rf_r2_train_orig = r2_score(rf_train_actual, rf_train_pred)\n",
        "rf_r2_test_orig = r2_score(rf_test_actual, rf_test_pred)\n",
        "\n",
        "rf_mae_train = mean_absolute_error(rf_train_actual, rf_train_pred)\n",
        "rf_mae_test = mean_absolute_error(rf_test_actual, rf_test_pred)\n",
        "rf_rmse_train = np.sqrt(mean_squared_error(rf_train_actual, rf_train_pred))\n",
        "rf_rmse_test = np.sqrt(mean_squared_error(rf_test_actual, rf_test_pred))\n",
        "\n",
        "print(\"\\nâš¡ Original-scale (RF):\")\n",
        "print(f\"   Train RÂ²: {rf_r2_train_orig:.6f}\")\n",
        "print(f\"   Test  RÂ²: {rf_r2_test_orig:.6f}\")\n",
        "\n",
        "print(\"\\nðŸ“ Error metrics (RF, Joules):\")\n",
        "print(f\"   Train MAE: {rf_mae_train:,.2f} J\")\n",
        "print(f\"   Test  MAE: {rf_mae_test:,.2f} J\")\n",
        "print(f\"   Train RMSE: {rf_rmse_train:,.2f} J\")\n",
        "print(f\"   Test  RMSE: {rf_rmse_test:,.2f} J\")\n",
        "\n",
        "if not np.isnan(mean_energy) and mean_energy != 0:\n",
        "    print(f\"   RF MAE as % of mean energy: {(rf_mae_test/mean_energy)*100:.2f}%\")\n",
        "\n",
        "# RF cross-val\n",
        "print(\"\\nðŸ”„ 5-Fold CV (RF, log-scale):\")\n",
        "try:\n",
        "    rf_cv_scores = cross_val_score(rf, X_scaled, y_log, cv=5, scoring='r2', n_jobs=-1)\n",
        "    print(f\"   CV RÂ²: {[f'{s:.4f}' for s in rf_cv_scores]}\")\n",
        "    print(f\"   Mean CV RÂ²: {rf_cv_scores.mean():.6f} Â±{rf_cv_scores.std():.6f}\")\n",
        "except Exception as e:\n",
        "    print(f\"   âš ï¸ RF CV failed: {e}\")\n",
        "\n",
        "# ==========================\n",
        "# FINAL SUMMARY\n",
        "# ==========================\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ðŸŽ‰ FINAL SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"âœ… Trained on {X_clean.shape[0]:,} clean samples\")\n",
        "print(f\"âœ… Using {X_clean.shape[1]} pure static features\")\n",
        "print(f\"âœ… Outliers removed (by features/rows): {removed_count:,}\")\n",
        "try:\n",
        "    print(f\"âœ… LightGBM Test RÂ² (log): {r2_test_log:.6f}\")\n",
        "    print(f\"âœ… LightGBM Test MAE (J): {mae_test:,.2f}\")\n",
        "except:\n",
        "    pass\n",
        "try:\n",
        "    print(f\"âœ… RF Test RÂ² (orig): {rf_r2_test_orig:.6f}\")\n",
        "except:\n",
        "    pass\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\nðŸ“‹ Final static features used:\")\n",
        "safe_print_cols(sorted(X.columns), max_show=200)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNro5v8xCcX5",
        "outputId": "78a6e59d-ee0a-43bc-dc7b-0da4e0bf23a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "PURE STATIC FEATURES MODEL - DEBUGGED\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š Initial dataset: 6,711 rows Ã— 107 columns\n",
            "âœ… After filtering executed_successfully: 1,567 rows (23.3% of pre-filter)\n",
            "âœ… After removing zero/NaN target: 1,567 rows\n",
            "\n",
            "ðŸ” Identifying PURE STATIC features...\n",
            "\n",
            "ðŸ“‹ Feature filtering summary:\n",
            "   Total columns: 107\n",
            "   Removed metadata columns: 4\n",
            "   Removed explicit dynamic features: 4\n",
            "   Removed suspicious keyword matches: 34\n",
            "   âœ… Candidate static features (pre-numeric): 64\n",
            "âš ï¸ Example removed suspicious columns:\n",
            "   - file_extension\n",
            "   - avg_line_length\n",
            "   - max_line_length\n",
            "   - min_line_length\n",
            "   - comment_ratio\n",
            "   - unique_indentation_levels\n",
            "   - avg_indentation\n",
            "   - max_indentation\n",
            "   - std_indentation\n",
            "   - functions_count\n",
            "   - async_functions_count\n",
            "   - lambda_functions\n",
            "   - list_comprehensions\n",
            "   - dict_comprehensions\n",
            "   - set_comprehensions\n",
            "   - generator_expressions\n",
            "   - binary_operations\n",
            "   - unary_operations\n",
            "   - boolean_operations\n",
            "   - max_nesting_depth\n",
            "   ... and 14 more\n",
            "\n",
            "ðŸ“ Feature matrix: 1,567 rows Ã— 64 features\n",
            "\n",
            "ðŸ§¹ Cleaning feature matrix (infs -> NaN, drop high-missing, fill medians)...\n",
            "âœ… After cleaning: 64 features remain\n",
            "\n",
            "ðŸ”§ Removing constant and very low-variance features...\n",
            "   Removed 2 constant features\n",
            "   After low-variance filtering: 58 features remain\n",
            "âœ… Final static feature count: 58\n",
            "\n",
            "ðŸ” Aggressive outlier removal (IsolationForest -> quantiles -> z-score)...\n",
            "   Initial samples: 1,567\n",
            "   After IsolationForest (10% removal): 1,410 samples\n",
            "   After quantile filtering (1%-97%): 1,352 samples\n",
            "   After z-score filtering (3.5Ïƒ): 1,030 samples\n",
            "âœ… Total outliers removed (feature-rows): 537 (34.3%)\n",
            "\n",
            "ðŸ“ˆ Applied log1p to target:\n",
            "   Original target range: [3.31, 386.81]\n",
            "   Log-transformed range: [1.46, 5.96]\n",
            "âœ… RobustScaler applied\n",
            "\n",
            "ðŸ“Š Train/Test split: train=824, test=206\n",
            "\n",
            "ðŸš€ Training LightGBM (features: 58)\n",
            "[100]\tvalid_0's rmse: 1.01257\tvalid_0's l2: 1.02529\n",
            "[200]\tvalid_0's rmse: 0.924541\tvalid_0's l2: 0.854777\n",
            "[300]\tvalid_0's rmse: 0.905864\tvalid_0's l2: 0.82059\n",
            "[400]\tvalid_0's rmse: 0.901552\tvalid_0's l2: 0.812797\n",
            "[500]\tvalid_0's rmse: 0.900023\tvalid_0's l2: 0.810042\n",
            "[600]\tvalid_0's rmse: 0.899998\tvalid_0's l2: 0.809997\n",
            "[700]\tvalid_0's rmse: 0.900124\tvalid_0's l2: 0.810224\n",
            "[800]\tvalid_0's rmse: 0.900807\tvalid_0's l2: 0.811453\n",
            "[900]\tvalid_0's rmse: 0.900633\tvalid_0's l2: 0.811141\n",
            "[1000]\tvalid_0's rmse: 0.900412\tvalid_0's l2: 0.810742\n",
            "[1100]\tvalid_0's rmse: 0.898753\tvalid_0's l2: 0.807757\n",
            "[1200]\tvalid_0's rmse: 0.897848\tvalid_0's l2: 0.80613\n",
            "[1300]\tvalid_0's rmse: 0.897409\tvalid_0's l2: 0.805342\n",
            "[1400]\tvalid_0's rmse: 0.896458\tvalid_0's l2: 0.803637\n",
            "[1500]\tvalid_0's rmse: 0.89584\tvalid_0's l2: 0.80253\n",
            "[1600]\tvalid_0's rmse: 0.895112\tvalid_0's l2: 0.801225\n",
            "[1700]\tvalid_0's rmse: 0.894547\tvalid_0's l2: 0.800214\n",
            "[1800]\tvalid_0's rmse: 0.894568\tvalid_0's l2: 0.800251\n",
            "[1900]\tvalid_0's rmse: 0.894539\tvalid_0's l2: 0.800199\n",
            "[2000]\tvalid_0's rmse: 0.894797\tvalid_0's l2: 0.800662\n",
            "\n",
            "================================================================================\n",
            "LIGHTGBM PERFORMANCE\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š Log-scale RÂ²:\n",
            "   Train RÂ²: 0.987514\n",
            "   Test  RÂ²: 0.598192\n",
            "\n",
            "âš¡ Original scale performance:\n",
            "   Train RÂ²: 0.927227\n",
            "   Test  RÂ²: 0.308607\n",
            "\n",
            "ðŸ“ Error metrics (Joules):\n",
            "   Train MAE: 5.39 J\n",
            "   Test  MAE: 27.19 J\n",
            "   Train RMSE: 16.78 J\n",
            "   Test  RMSE: 56.56 J\n",
            "   Mean energy (test): 46.98 J\n",
            "   MAE as % of mean: 57.87%\n",
            "\n",
            "ðŸŽ¯ Top LightGBM features:\n",
            "   file_index                                 2996.0\n",
            "   std_line_length                            1960.0\n",
            "   brackets_count                             1477.0\n",
            "   total_ast_nodes                            1469.0\n",
            "   single_quotes                              1357.0\n",
            "   total_quotes                               1355.0\n",
            "   parentheses_count                          1347.0\n",
            "   double_quotes                              1243.0\n",
            "   code_lines                                 1210.0\n",
            "   colon_count                                1176.0\n",
            "   assignments                                1158.0\n",
            "   unique_variables                           1156.0\n",
            "   comment_lines                              1152.0\n",
            "   cyclomatic_complexity                      1043.0\n",
            "   braces_count                               1009.0\n",
            "   keyword_from_count                          991.0\n",
            "   blank_lines                                 986.0\n",
            "   total_characters                            958.0\n",
            "   keyword_import_count                        939.0\n",
            "   total_lines                                 853.0\n",
            "\n",
            "ðŸ”„ 5-Fold CV (LGBM, log-scale):\n",
            "   CV RÂ²: ['0.6678', '0.5679', '-0.3622', '0.5137', '0.6335']\n",
            "   Mean CV RÂ²: 0.404146 Â±0.386854\n",
            "\n",
            "ðŸŒ² Training RandomForestRegressor (max_features='sqrt')\n",
            "\n",
            "================================================================================\n",
            "RANDOM FOREST PERFORMANCE\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š Log-scale RÂ² (RF):\n",
            "   Train RÂ²: 0.928584\n",
            "   Test  RÂ²: 0.419942\n",
            "\n",
            "âš¡ Original-scale (RF):\n",
            "   Train RÂ²: 0.680987\n",
            "   Test  RÂ²: 0.020877\n",
            "\n",
            "ðŸ“ Error metrics (RF, Joules):\n",
            "   Train MAE: 14.67 J\n",
            "   Test  MAE: 34.00 J\n",
            "   Train RMSE: 35.14 J\n",
            "   Test  RMSE: 67.31 J\n",
            "   RF MAE as % of mean energy: 72.36%\n",
            "\n",
            "ðŸ”„ 5-Fold CV (RF, log-scale):\n",
            "   CV RÂ²: ['0.5015', '0.5097', '-0.3143', '0.4841', '0.5535']\n",
            "   Mean CV RÂ²: 0.346899 Â±0.331394\n",
            "\n",
            "================================================================================\n",
            "ðŸŽ‰ FINAL SUMMARY\n",
            "================================================================================\n",
            "âœ… Trained on 1,030 clean samples\n",
            "âœ… Using 58 pure static features\n",
            "âœ… Outliers removed (by features/rows): 537\n",
            "âœ… LightGBM Test RÂ² (log): 0.598192\n",
            "âœ… LightGBM Test MAE (J): 27.19\n",
            "âœ… RF Test RÂ² (orig): 0.020877\n",
            "================================================================================\n",
            "\n",
            "ðŸ“‹ Final static features used:\n",
            "   - assert_statements\n",
            "   - assignments\n",
            "   - async_for_loops\n",
            "   - async_with_statements\n",
            "   - augmented_assignments\n",
            "   - blank_lines\n",
            "   - braces_count\n",
            "   - brackets_count\n",
            "   - classes_count\n",
            "   - code_lines\n",
            "   - colon_count\n",
            "   - comment_lines\n",
            "   - comparisons\n",
            "   - cyclomatic_complexity\n",
            "   - double_quotes\n",
            "   - file_index\n",
            "   - file_size_bytes\n",
            "   - file_size_kb\n",
            "   - for_loops\n",
            "   - if_statements\n",
            "   - imports_count\n",
            "   - keyword_async_count\n",
            "   - keyword_await_count\n",
            "   - keyword_class_count\n",
            "   - keyword_def_count\n",
            "   - keyword_elif_count\n",
            "   - keyword_else_count\n",
            "   - keyword_except_count\n",
            "   - keyword_finally_count\n",
            "   - keyword_for_count\n",
            "   - keyword_from_count\n",
            "   - keyword_global_count\n",
            "   - keyword_if_count\n",
            "   - keyword_import_count\n",
            "   - keyword_lambda_count\n",
            "   - keyword_return_count\n",
            "   - keyword_try_count\n",
            "   - keyword_while_count\n",
            "   - keyword_with_count\n",
            "   - keyword_yield_count\n",
            "   - parentheses_count\n",
            "   - raise_statements\n",
            "   - return_statements\n",
            "   - semicolon_count\n",
            "   - single_quotes\n",
            "   - std_line_length\n",
            "   - total_ast_nodes\n",
            "   - total_characters\n",
            "   - total_lines\n",
            "   - total_quotes\n",
            "   - triple_quotes\n",
            "   - try_blocks\n",
            "   - unique_class_names\n",
            "   - unique_imports\n",
            "   - unique_variables\n",
            "   - while_loops\n",
            "   - with_statements\n",
            "   - yield_statements\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# STATIC + DYNAMIC\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
        "from sklearn.preprocessing import RobustScaler, PowerTransformer\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
        "from lightgbm import LGBMRegressor\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "DATASET_PATH = '/content/ml_code_dataset.csv'\n",
        "\n",
        "# Comprehensive dynamic features list\n",
        "DYNAMIC_FEATURES = [\n",
        "    'execution_time', 'runtime_seconds', 'elapsed_time', 'duration',\n",
        "    'memory_peak_mb', 'memory_avg_mb', 'memory_usage', 'memory_allocated',\n",
        "    'memory_reserved', 'memory_max', 'ram_usage', 'heap_size',\n",
        "    'cpu_percent_avg', 'cpu_percent_peak', 'cpu_usage', 'cpu_time',\n",
        "    'user_time', 'system_time', 'cpu_cycles',\n",
        "    'disk_read_mb', 'disk_write_mb', 'io_read', 'io_write',\n",
        "    'disk_operations', 'read_bytes', 'write_bytes',\n",
        "    'co2_emissions_g', 'power_draw_watts', 'energy_pkg_joules',\n",
        "    'energy_ram_joules', 'energy_consumption', 'carbon_footprint',\n",
        "    'gpu_used', 'gpu_memory', 'gpu_utilization', 'cuda_memory',\n",
        "    'network_sent_mb', 'network_recv_mb', 'bytes_sent', 'bytes_received',\n",
        "    'packets_sent', 'packets_received',\n",
        "    'process_count', 'thread_count', 'context_switches', 'num_threads',\n",
        "    'num_processes', 'child_processes',\n",
        "    'cache_misses', 'cache_hits', 'instructions_retired', 'cycles_elapsed',\n",
        "    'branch_misses', 'page_faults', 'tlb_misses',\n",
        "    'runtime_errors', 'memory_errors', 'exception_count', 'error_count',\n",
        "    'warning_count', 'num_exceptions',\n",
        "    'output_size', 'return_value', 'stdout_length', 'stderr_length',\n",
        "    'output_lines', 'print_statements_executed',\n",
        "    'system_load', 'available_memory', 'free_disk', 'temperature'\n",
        "]\n",
        "\n",
        "METADATA_COLS = [\n",
        "    'file_path', 'file_name', 'repo_name', 'repo_url', 'timestamp',\n",
        "    'executed_successfully', 'syntax_valid', 'run_id', 'index', 'id',\n",
        "    'file_id', 'repository', 'author', 'date_created', 'date_modified'\n",
        "]\n",
        "\n",
        "TARGET = 'energy_consumption_joules'\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: LOAD AND FILTER\n",
        "# ============================================================================\n",
        "print(\"=\" * 80)\n",
        "print(\"ENHANCED STATIC FEATURES MODEL - MAXIMUM RÂ² OPTIMIZATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "print(f\"\\nðŸ“Š Initial Dataset: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
        "\n",
        "# Filter for successfully executed files\n",
        "if 'executed_successfully' in df.columns:\n",
        "    df = df[df['executed_successfully'] == True].copy()\n",
        "    print(f\"âœ… After filtering executed_successfully: {df.shape[0]:,} rows\")\n",
        "\n",
        "# Remove zero/NaN energy\n",
        "df = df[df[TARGET] > 0].copy()\n",
        "df = df.dropna(subset=[TARGET])\n",
        "print(f\"âœ… After removing zero/NaN energy: {df.shape[0]:,} rows\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: IDENTIFY STATIC FEATURES (KEEP MORE FEATURES)\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸ” Identifying static features with RELAXED filtering...\")\n",
        "\n",
        "all_cols = df.columns.tolist()\n",
        "\n",
        "# First pass: Remove obvious dynamic and metadata\n",
        "remaining_cols = [\n",
        "    col for col in all_cols\n",
        "    if col not in DYNAMIC_FEATURES + METADATA_COLS + [TARGET]\n",
        "]\n",
        "\n",
        "# Second pass: Only remove columns with STRONG dynamic keywords\n",
        "# BE MORE PERMISSIVE - keep features like avg_indentation, max_nesting_depth\n",
        "strong_dynamic_keywords = [\n",
        "    'execution', 'runtime', 'elapsed',  # time-related\n",
        "    'memory_', 'ram_', 'heap_',  # memory-related (but not 'memory' alone)\n",
        "    'cpu_percent', 'cpu_usage', 'cpu_time',  # cpu metrics\n",
        "    'disk_read', 'disk_write', 'io_read', 'io_write',  # disk I/O\n",
        "    'network_', 'bytes_sent', 'bytes_received',  # network\n",
        "    'process_count', 'thread_count',  # process metrics\n",
        "    'exception_count', 'error_count',  # runtime errors\n",
        "    'gpu_', 'cuda_',  # gpu\n",
        "    'cache_miss', 'cache_hit', 'instructions_retired'  # hardware counters\n",
        "]\n",
        "\n",
        "static_features = []\n",
        "removed_suspicious = []\n",
        "\n",
        "for col in remaining_cols:\n",
        "    col_lower = col.lower()\n",
        "    is_dynamic = False\n",
        "\n",
        "    for keyword in strong_dynamic_keywords:\n",
        "        if keyword in col_lower:\n",
        "            is_dynamic = True\n",
        "            removed_suspicious.append(col)\n",
        "            break\n",
        "\n",
        "    if not is_dynamic:\n",
        "        static_features.append(col)\n",
        "\n",
        "print(f\"\\nðŸ“‹ Feature Filtering (Relaxed):\")\n",
        "print(f\"   Removed explicit dynamic: {len([c for c in all_cols if c in DYNAMIC_FEATURES])}\")\n",
        "print(f\"   Removed suspicious: {len(removed_suspicious)}\")\n",
        "print(f\"   âœ… STATIC FEATURES: {len(static_features)}\")\n",
        "\n",
        "X = df[static_features].copy()\n",
        "y = df[TARGET].copy()\n",
        "\n",
        "print(f\"\\nðŸ“ Feature Matrix: {X.shape[0]:,} rows Ã— {X.shape[1]} features\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: LESS AGGRESSIVE CLEANING\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸ§¹ Data cleaning...\")\n",
        "\n",
        "# Handle infinite values\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "# Remove columns with >70% missing (more permissive)\n",
        "missing_pct = X.isna().sum() / len(X)\n",
        "cols_to_drop = missing_pct[missing_pct > 0.7].index.tolist()\n",
        "if cols_to_drop:\n",
        "    print(f\"   Dropping {len(cols_to_drop)} columns with >70% missing\")\n",
        "    X = X.drop(columns=cols_to_drop)\n",
        "\n",
        "# Fill NaN with median\n",
        "for col in X.columns:\n",
        "    if X[col].dtype in ['float64', 'int64', 'float32', 'int32']:\n",
        "        if X[col].isna().sum() > 0:\n",
        "            X[col].fillna(X[col].median(), inplace=True)\n",
        "\n",
        "# Remove constant features only\n",
        "constant_cols = [col for col in X.columns if X[col].nunique() <= 1]\n",
        "if constant_cols:\n",
        "    X = X.drop(columns=constant_cols)\n",
        "    print(f\"   Removed {len(constant_cols)} constant features\")\n",
        "\n",
        "# Keep more features - only remove bottom 2% variance\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "if X.shape[1] > 10:\n",
        "    variance_threshold = VarianceThreshold(threshold=0.0)\n",
        "    variance_threshold.fit(X)\n",
        "    feature_variances = variance_threshold.variances_\n",
        "    variance_cutoff = np.percentile(feature_variances, 2)  # Changed from 5%\n",
        "    low_var_mask = feature_variances > variance_cutoff\n",
        "    X = X.loc[:, low_var_mask]\n",
        "    print(f\"   After variance filtering: {X.shape[1]} features\")\n",
        "\n",
        "print(f\"âœ… Features after cleaning: {X.shape[1]}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: SMARTER OUTLIER REMOVAL (LESS AGGRESSIVE)\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸ” Smarter Outlier Removal...\")\n",
        "print(f\"   Initial size: {X.shape[0]:,} samples\")\n",
        "\n",
        "# Only remove 5% most anomalous (was 10%)\n",
        "iso_forest = IsolationForest(\n",
        "    contamination=0.05,  # Less aggressive\n",
        "    random_state=42,\n",
        "    n_estimators=200,\n",
        "    max_samples='auto'\n",
        ")\n",
        "outlier_pred = iso_forest.fit_predict(X)\n",
        "mask_iso = outlier_pred == 1\n",
        "\n",
        "X_clean = X[mask_iso].copy()\n",
        "y_clean = y[mask_iso].copy()\n",
        "print(f\"   After IsolationForest (5%): {X_clean.shape[0]:,} samples\")\n",
        "\n",
        "# Less aggressive quantile filtering (2%-98%)\n",
        "q_low = y_clean.quantile(0.02)  # Changed from 0.01\n",
        "q_high = y_clean.quantile(0.98)  # Changed from 0.97\n",
        "mask_quantile = (y_clean >= q_low) & (y_clean <= q_high)\n",
        "\n",
        "X_clean = X_clean[mask_quantile]\n",
        "y_clean = y_clean[mask_quantile]\n",
        "print(f\"   After quantile filtering (2%-98%): {X_clean.shape[0]:,} samples\")\n",
        "\n",
        "# More permissive z-score (4.0 instead of 3.5)\n",
        "z_scores = np.abs((X_clean - X_clean.mean()) / (X_clean.std() + 1e-10))\n",
        "mask_z = (z_scores < 4.0).all(axis=1)\n",
        "\n",
        "X_clean = X_clean[mask_z]\n",
        "y_clean = y_clean[mask_z]\n",
        "print(f\"   After z-score filtering (4.0Ïƒ): {X_clean.shape[0]:,} samples\")\n",
        "print(f\"âœ… Outliers removed: {X.shape[0] - X_clean.shape[0]:,} ({(1-X_clean.shape[0]/X.shape[0])*100:.1f}%)\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: FEATURE ENGINEERING\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸ”§ Feature Engineering...\")\n",
        "\n",
        "X_eng = X_clean.copy()\n",
        "\n",
        "# Polynomial features for top predictors\n",
        "if 'total_lines' in X_eng.columns and 'cyclomatic_complexity' in X_eng.columns:\n",
        "    X_eng['lines_x_complexity'] = X_eng['total_lines'] * X_eng['cyclomatic_complexity']\n",
        "\n",
        "if 'code_lines' in X_eng.columns and 'total_ast_nodes' in X_eng.columns:\n",
        "    X_eng['code_x_ast'] = X_eng['code_lines'] * X_eng['total_ast_nodes']\n",
        "\n",
        "if 'total_lines' in X_eng.columns:\n",
        "    X_eng['lines_squared'] = X_eng['total_lines'] ** 2\n",
        "    X_eng['lines_log'] = np.log1p(X_eng['total_lines'])\n",
        "\n",
        "if 'cyclomatic_complexity' in X_eng.columns:\n",
        "    X_eng['complexity_squared'] = X_eng['cyclomatic_complexity'] ** 2\n",
        "    X_eng['complexity_log'] = np.log1p(X_eng['cyclomatic_complexity'])\n",
        "\n",
        "# Ratios\n",
        "if 'code_lines' in X_eng.columns and 'total_lines' in X_eng.columns:\n",
        "    X_eng['code_to_total_ratio'] = X_eng['code_lines'] / (X_eng['total_lines'] + 1)\n",
        "\n",
        "if 'comment_lines' in X_eng.columns and 'code_lines' in X_eng.columns:\n",
        "    X_eng['comment_to_code_ratio'] = X_eng['comment_lines'] / (X_eng['code_lines'] + 1)\n",
        "\n",
        "if 'cyclomatic_complexity' in X_eng.columns and 'total_lines' in X_eng.columns:\n",
        "    X_eng['complexity_per_line_ratio'] = X_eng['cyclomatic_complexity'] / (X_eng['total_lines'] + 1)\n",
        "\n",
        "print(f\"âœ… Engineered features: {X_eng.shape[1]} total features\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: LOG TRANSFORM TARGET\n",
        "# ============================================================================\n",
        "y_log = np.log1p(y_clean)\n",
        "print(f\"\\nðŸ“ˆ Log-transformed target: [{y_log.min():.2f}, {y_log.max():.2f}]\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: FEATURE SELECTION - Keep Best Features\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸŽ¯ Feature Selection...\")\n",
        "\n",
        "# Use mutual information to select best features\n",
        "selector = SelectKBest(score_func=mutual_info_regression, k=min(45, X_eng.shape[1]))\n",
        "selector.fit(X_eng, y_log)\n",
        "\n",
        "# Get selected features\n",
        "selected_mask = selector.get_support()\n",
        "X_selected = X_eng.loc[:, selected_mask]\n",
        "selected_features = X_eng.columns[selected_mask].tolist()\n",
        "\n",
        "print(f\"âœ… Selected top {X_selected.shape[1]} features using mutual information\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 8: POWER TRANSFORM + SCALING\n",
        "# ============================================================================\n",
        "# PowerTransformer makes features more Gaussian\n",
        "power_transformer = PowerTransformer(method='yeo-johnson', standardize=True)\n",
        "X_transformed = pd.DataFrame(\n",
        "    power_transformer.fit_transform(X_selected),\n",
        "    columns=X_selected.columns,\n",
        "    index=X_selected.index\n",
        ")\n",
        "\n",
        "print(f\"âœ… Applied PowerTransformer (Yeo-Johnson) + standardization\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 9: TRAIN-TEST SPLIT\n",
        "# ============================================================================\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_transformed, y_log, test_size=0.2, random_state=42, shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"\\nðŸ“Š Train-Test Split:\")\n",
        "print(f\"   Training: {X_train.shape[0]:,} samples\")\n",
        "print(f\"   Test: {X_test.shape[0]:,} samples\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 10: TRAIN OPTIMIZED MODEL WITH REGULARIZATION\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸš€ Training Regularized LightGBM...\")\n",
        "\n",
        "# More regularization to prevent overfitting\n",
        "model = LGBMRegressor(\n",
        "    objective='regression',\n",
        "    n_estimators=1500,  # Reduced\n",
        "    learning_rate=0.005,  # Lower learning rate\n",
        "    num_leaves=20,  # Reduced from 31\n",
        "    max_depth=6,  # Reduced from 8\n",
        "    min_child_samples=30,  # Increased\n",
        "    min_child_weight=0.01,\n",
        "    subsample=0.7,  # More aggressive subsampling\n",
        "    colsample_bytree=0.7,\n",
        "    reg_alpha=1.0,  # Stronger L1 regularization\n",
        "    reg_lambda=1.0,  # Stronger L2 regularization\n",
        "    min_split_gain=0.01,\n",
        "    random_state=42,\n",
        "    verbose=-1,\n",
        "    n_jobs=-1,\n",
        "    force_col_wise=True\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "    X_train, y_train,\n",
        "    eval_set=[(X_test, y_test)],\n",
        "    eval_metric='rmse'\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 11: EVALUATE\n",
        "# ============================================================================\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(\"MODEL PERFORMANCE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "y_train_pred_log = model.predict(X_train)\n",
        "y_test_pred_log = model.predict(X_test)\n",
        "\n",
        "r2_train_log = r2_score(y_train, y_train_pred_log)\n",
        "r2_test_log = r2_score(y_test, y_test_pred_log)\n",
        "\n",
        "print(f\"\\nðŸ“Š Log-Scale Performance:\")\n",
        "print(f\"   Training RÂ²: {r2_train_log:.6f}\")\n",
        "print(f\"   Test RÂ²:     {r2_test_log:.6f}\")\n",
        "print(f\"   Overfitting Gap: {r2_train_log - r2_test_log:.6f}\")\n",
        "\n",
        "# Original scale\n",
        "y_train_actual = np.expm1(y_train)\n",
        "y_train_pred = np.expm1(y_train_pred_log)\n",
        "y_test_actual = np.expm1(y_test)\n",
        "y_test_pred = np.expm1(y_test_pred_log)\n",
        "\n",
        "r2_train_original = r2_score(y_train_actual, y_train_pred)\n",
        "r2_test_original = r2_score(y_test_actual, y_test_pred)\n",
        "\n",
        "print(f\"\\nâš¡ Original Joules Scale:\")\n",
        "print(f\"   Training RÂ²: {r2_train_original:.6f}\")\n",
        "print(f\"   Test RÂ²:     {r2_test_original:.6f}\")\n",
        "\n",
        "mae_test = mean_absolute_error(y_test_actual, y_test_pred)\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test_actual, y_test_pred))\n",
        "mean_energy = y_test_actual.mean()\n",
        "\n",
        "print(f\"\\nðŸ“ Error Metrics:\")\n",
        "print(f\"   Test MAE:  {mae_test:,.2f} J ({(mae_test/mean_energy)*100:.1f}% of mean)\")\n",
        "print(f\"   Test RMSE: {rmse_test:,.2f} J\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 12: CROSS-VALIDATION\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸ”„ 5-Fold Cross-Validation:\")\n",
        "cv_scores = cross_val_score(\n",
        "    model, X_transformed, y_log, cv=5, scoring='r2', n_jobs=-1\n",
        ")\n",
        "print(f\"   CV RÂ² Scores: {[f'{s:.4f}' for s in cv_scores]}\")\n",
        "print(f\"   Mean CV RÂ²: {cv_scores.mean():.6f} (Â±{cv_scores.std():.6f})\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 13: FEATURE IMPORTANCE\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸŽ¯ Top 15 Most Important Features:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "for idx, row in feature_importance.head(15).iterrows():\n",
        "    print(f\"   {row['feature']:35s} {row['importance']:8.1f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL SUMMARY\n",
        "# ============================================================================\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(\"ðŸŽ‰ FINAL RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"âœ… Training samples: {X_train.shape[0]:,}\")\n",
        "print(f\"âœ… Features used: {X_transformed.shape[1]}\")\n",
        "print(f\"âœ… Test RÂ² (Log): {r2_test_log:.6f}\")\n",
        "print(f\"âœ… Test RÂ² (Original): {r2_test_original:.6f}\")\n",
        "print(f\"âœ… CV RÂ²: {cv_scores.mean():.6f} Â±{cv_scores.std():.4f}\")\n",
        "print(f\"âœ… Test MAE: {mae_test:,.2f} J ({(mae_test/mean_energy)*100:.1f}% of mean)\")\n",
        "print(f\"âœ… Overfitting: {r2_train_log - r2_test_log:.4f} (lower is better)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nðŸ’¡ Key Improvements:\")\n",
        "print(f\"   â€¢ Less aggressive outlier removal (kept more data)\")\n",
        "print(f\"   â€¢ Relaxed feature filtering (kept useful features)\")\n",
        "print(f\"   â€¢ Feature engineering (interactions, polynomials)\")\n",
        "print(f\"   â€¢ Feature selection (mutual information)\")\n",
        "print(f\"   â€¢ PowerTransformer for better distributions\")\n",
        "print(f\"   â€¢ Stronger regularization (reduced overfitting)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Nputq5xDZlg",
        "outputId": "20a3221d-e150-4a66-cf37-37a5c5ad8a8d",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "ENHANCED STATIC FEATURES MODEL - MAXIMUM RÂ² OPTIMIZATION\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š Initial Dataset: 6,711 rows Ã— 107 columns\n",
            "âœ… After filtering executed_successfully: 1,567 rows\n",
            "âœ… After removing zero/NaN energy: 1,567 rows\n",
            "\n",
            "ðŸ” Identifying static features with RELAXED filtering...\n",
            "\n",
            "ðŸ“‹ Feature Filtering (Relaxed):\n",
            "   Removed explicit dynamic: 4\n",
            "   Removed suspicious: 3\n",
            "   âœ… STATIC FEATURES: 95\n",
            "\n",
            "ðŸ“ Feature Matrix: 1,567 rows Ã— 95 features\n",
            "\n",
            "ðŸ§¹ Data cleaning...\n",
            "   Dropping 2 columns with >70% missing\n",
            "   Removed 3 constant features\n",
            "   After variance filtering: 88 features\n",
            "âœ… Features after cleaning: 88\n",
            "\n",
            "ðŸ” Smarter Outlier Removal...\n",
            "   Initial size: 1,567 samples\n",
            "   After IsolationForest (5%): 1,488 samples\n",
            "   After quantile filtering (2%-98%): 1,428 samples\n",
            "   After z-score filtering (4.0Ïƒ): 1,089 samples\n",
            "âœ… Outliers removed: 478 (30.5%)\n",
            "\n",
            "ðŸ”§ Feature Engineering...\n",
            "âœ… Engineered features: 97 total features\n",
            "\n",
            "ðŸ“ˆ Log-transformed target: [1.47, 6.11]\n",
            "\n",
            "ðŸŽ¯ Feature Selection...\n",
            "âœ… Selected top 45 features using mutual information\n",
            "âœ… Applied PowerTransformer (Yeo-Johnson) + standardization\n",
            "\n",
            "ðŸ“Š Train-Test Split:\n",
            "   Training: 871 samples\n",
            "   Test: 218 samples\n",
            "\n",
            "ðŸš€ Training Regularized LightGBM...\n",
            "\n",
            "================================================================================\n",
            "MODEL PERFORMANCE\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š Log-Scale Performance:\n",
            "   Training RÂ²: 0.994920\n",
            "   Test RÂ²:     0.986287\n",
            "   Overfitting Gap: 0.008634\n",
            "\n",
            "âš¡ Original Joules Scale:\n",
            "   Training RÂ²: 0.960007\n",
            "   Test RÂ²:     0.912200\n",
            "\n",
            "ðŸ“ Error Metrics:\n",
            "   Test MAE:  7.53 J (16.7% of mean)\n",
            "   Test RMSE: 20.80 J\n",
            "\n",
            "ðŸ”„ 5-Fold Cross-Validation:\n",
            "   CV RÂ² Scores: ['0.9847', '0.9807', '0.9780', '0.9854', '0.9869']\n",
            "   Mean CV RÂ²: 0.983129 (Â±0.003293)\n",
            "\n",
            "ðŸŽ¯ Top 15 Most Important Features:\n",
            "------------------------------------------------------------\n",
            "   energy_per_line                       4030.0\n",
            "   code_lines                            1984.0\n",
            "   time_per_line                         1683.0\n",
            "   total_lines                           1246.0\n",
            "   complexity_per_line                    668.0\n",
            "   lines_x_complexity                     595.0\n",
            "   code_density                           500.0\n",
            "   control_flow_ratio                     490.0\n",
            "   total_characters                       451.0\n",
            "   code_x_ast                             419.0\n",
            "   lines_squared                          392.0\n",
            "   keyword_import_count                   299.0\n",
            "   total_ast_nodes                        290.0\n",
            "   std_indentation                        285.0\n",
            "   complexity_per_line_ratio              283.0\n",
            "\n",
            "================================================================================\n",
            "ðŸŽ‰ FINAL RESULTS\n",
            "================================================================================\n",
            "âœ… Training samples: 871\n",
            "âœ… Features used: 45\n",
            "âœ… Test RÂ² (Log): 0.986287\n",
            "âœ… Test RÂ² (Original): 0.912200\n",
            "âœ… CV RÂ²: 0.983129 Â±0.0033\n",
            "âœ… Test MAE: 7.53 J (16.7% of mean)\n",
            "âœ… Overfitting: 0.0086 (lower is better)\n",
            "================================================================================\n",
            "\n",
            "ðŸ’¡ Key Improvements:\n",
            "   â€¢ Less aggressive outlier removal (kept more data)\n",
            "   â€¢ Relaxed feature filtering (kept useful features)\n",
            "   â€¢ Feature engineering (interactions, polynomials)\n",
            "   â€¢ Feature selection (mutual information)\n",
            "   â€¢ PowerTransformer for better distributions\n",
            "   â€¢ Stronger regularization (reduced overfitting)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.linear_model import Ridge, ElasticNet\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "DATASET_PATH = '/content/ml_code_dataset.csv'\n",
        "\n",
        "# ============================================================================\n",
        "# FEATURE CATEGORIZATION - STRICT LEAKAGE PREVENTION\n",
        "# ============================================================================\n",
        "\n",
        "# TARGET (what we're predicting)\n",
        "TARGET = 'energy_consumption_joules'\n",
        "\n",
        "# LEAKAGE FEATURES - MUST BE REMOVED\n",
        "# These are derived FROM the target or highly correlated outcomes\n",
        "LEAKAGE_FEATURES = [\n",
        "    'co2_emissions_g',           # Direct calculation from energy\n",
        "    'energy_per_line',           # Derived: energy / lines\n",
        "    'time_per_line',             # Derived: time / lines\n",
        "    'co2_per_line',              # Derived: co2 / lines\n",
        "    'energy_pkg_joules',         # Alternative energy measurement\n",
        "    'energy_ram_joules',         # Alternative energy measurement\n",
        "    'power_draw_watts',          # Direct energy measurement\n",
        "]\n",
        "\n",
        "# DYNAMIC FEATURES - CAN USE (measured during execution, but not derived from energy)\n",
        "DYNAMIC_FEATURES = [\n",
        "    'execution_time',            # Independent measurement\n",
        "    'cpu_percent_avg',           # Independent measurement\n",
        "    'cpu_percent_max',           # Independent measurement\n",
        "    'memory_usage_mb',           # Independent measurement\n",
        "    'memory_peak_mb',            # Independent measurement\n",
        "]\n",
        "\n",
        "# GRAY FEATURES - CAN USE\n",
        "GRAY_FEATURES = [\n",
        "    'executed_successfully', 'execution_error', 'exit_code',\n",
        "    'syntax_valid', 'parse_error', 'syntax_error_line', 'syntax_error_type'\n",
        "]\n",
        "\n",
        "# STATIC FEATURES - CAN USE\n",
        "STATIC_FEATURES = [\n",
        "    'total_lines', 'code_lines', 'blank_lines', 'total_characters',\n",
        "    'file_size_bytes', 'file_size_kb', 'avg_line_length', 'max_line_length',\n",
        "    'min_line_length', 'std_line_length', 'comment_lines', 'single_quotes',\n",
        "    'double_quotes', 'triple_quotes', 'total_quotes', 'parentheses_count',\n",
        "    'brackets_count', 'braces_count', 'semicolon_count', 'colon_count',\n",
        "    'unique_indentation_levels', 'avg_indentation', 'max_indentation',\n",
        "    'std_indentation', 'keyword_def_count', 'keyword_class_count',\n",
        "    'keyword_import_count', 'keyword_from_count', 'keyword_if_count',\n",
        "    'keyword_else_count', 'keyword_elif_count', 'keyword_for_count',\n",
        "    'keyword_while_count', 'keyword_try_count', 'keyword_except_count',\n",
        "    'keyword_finally_count', 'keyword_with_count', 'keyword_return_count',\n",
        "    'keyword_yield_count', 'keyword_lambda_count', 'keyword_async_count',\n",
        "    'keyword_await_count', 'keyword_global_count', 'keyword_nonlocal_count',\n",
        "    'functions_count', 'async_functions_count', 'classes_count',\n",
        "    'imports_count', 'if_statements', 'for_loops', 'while_loops',\n",
        "    'async_for_loops', 'try_blocks', 'with_statements', 'async_with_statements',\n",
        "    'lambda_functions', 'list_comprehensions', 'dict_comprehensions',\n",
        "    'set_comprehensions', 'generator_expressions', 'binary_operations',\n",
        "    'unary_operations', 'boolean_operations', 'comparisons', 'decorators',\n",
        "    'assignments', 'augmented_assignments', 'return_statements',\n",
        "    'yield_statements', 'raise_statements', 'assert_statements',\n",
        "    'cyclomatic_complexity', 'max_nesting_depth', 'unique_function_names',\n",
        "    'unique_class_names', 'unique_imports', 'unique_variables',\n",
        "    'total_ast_nodes', 'comment_ratio', 'control_flow_ratio',\n",
        "    'function_ratio', 'class_ratio', 'code_density', 'complexity_per_line',\n",
        "    'functions_per_line', 'avg_complexity_per_function'\n",
        "]\n",
        "\n",
        "# METADATA - EXCLUDE\n",
        "METADATA_COLS = [\n",
        "    'file_path', 'file_name', 'repo_name', 'repo_url', 'timestamp',\n",
        "    'run_id', 'index', 'id', 'file_id', 'repository', 'author',\n",
        "    'date_created', 'date_modified', 'file_index'\n",
        "]\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: LOAD DATA\n",
        "# ============================================================================\n",
        "print(\"=\" * 80)\n",
        "print(\"STATIC + DYNAMIC FEATURES MODEL (ZERO LEAKAGE)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "print(f\"\\nðŸ“Š Initial Dataset: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
        "\n",
        "if 'executed_successfully' in df.columns:\n",
        "    df = df[df['executed_successfully'] == True].copy()\n",
        "\n",
        "df = df[df[TARGET] > 0].copy()\n",
        "df = df.dropna(subset=[TARGET])\n",
        "print(f\"âœ… Valid samples: {df.shape[0]:,} rows\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: BUILD FEATURE SET (PREVENT LEAKAGE!)\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸ”’ Building feature set with STRICT leakage prevention...\")\n",
        "\n",
        "available_cols = df.columns.tolist()\n",
        "\n",
        "# Collect all usable features\n",
        "usable_features = []\n",
        "for col in available_cols:\n",
        "    # Include if it's static, dynamic, or gray\n",
        "    if col in STATIC_FEATURES or col in DYNAMIC_FEATURES or col in GRAY_FEATURES:\n",
        "        # Exclude if it's target, metadata, or leakage\n",
        "        if col not in [TARGET] + METADATA_COLS + LEAKAGE_FEATURES:\n",
        "            usable_features.append(col)\n",
        "\n",
        "# Categorize features\n",
        "static_used = [f for f in usable_features if f in STATIC_FEATURES]\n",
        "dynamic_used = [f for f in usable_features if f in DYNAMIC_FEATURES]\n",
        "gray_used = [f for f in usable_features if f in GRAY_FEATURES]\n",
        "\n",
        "print(f\"\\nðŸ“‹ Feature Breakdown:\")\n",
        "print(f\"   Static features: {len(static_used)}\")\n",
        "print(f\"   Dynamic features: {len(dynamic_used)}\")\n",
        "print(f\"   Gray features: {len(gray_used)}\")\n",
        "print(f\"   âœ… Total usable: {len(usable_features)}\")\n",
        "\n",
        "# Verify no leakage\n",
        "leaked = [f for f in usable_features if f in LEAKAGE_FEATURES]\n",
        "if leaked:\n",
        "    print(f\"   âŒ ERROR: Leaked features found: {leaked}\")\n",
        "    exit()\n",
        "else:\n",
        "    print(f\"   âœ… No leakage detected!\")\n",
        "\n",
        "# Show dynamic features being used\n",
        "if dynamic_used:\n",
        "    print(f\"\\nðŸ”¥ Dynamic features included:\")\n",
        "    for feat in dynamic_used:\n",
        "        print(f\"      â€¢ {feat}\")\n",
        "\n",
        "X = df[usable_features].copy()\n",
        "y = df[TARGET].copy()\n",
        "\n",
        "print(f\"\\nðŸ“ Feature Matrix: {X.shape[0]:,} rows Ã— {X.shape[1]} features\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: MINIMAL CLEANING\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸ§¹ Data cleaning...\")\n",
        "\n",
        "# Handle infinite/NaN\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "for col in X.columns:\n",
        "    if X[col].dtype in ['float64', 'int64']:\n",
        "        if X[col].isna().sum() > 0:\n",
        "            X[col].fillna(X[col].median(), inplace=True)\n",
        "\n",
        "# Remove constant features\n",
        "constant_cols = [col for col in X.columns if X[col].nunique() <= 1]\n",
        "if constant_cols:\n",
        "    X = X.drop(columns=constant_cols)\n",
        "    print(f\"   Removed {len(constant_cols)} constant features\")\n",
        "\n",
        "# Very minimal outlier removal\n",
        "q_low = y.quantile(0.005)\n",
        "q_high = y.quantile(0.995)\n",
        "mask = (y >= q_low) & (y <= q_high)\n",
        "X = X[mask]\n",
        "y = y[mask]\n",
        "\n",
        "print(f\"âœ… Clean data: {X.shape[0]:,} samples, {X.shape[1]} features\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: FEATURE ENGINEERING\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸ”§ Feature Engineering...\")\n",
        "\n",
        "X_eng = X.copy()\n",
        "\n",
        "# === INTERACTIONS BETWEEN STATIC AND DYNAMIC ===\n",
        "print(\"   â€¢ Static Ã— Dynamic interactions...\")\n",
        "\n",
        "# Execution time interactions\n",
        "if 'execution_time' in X_eng.columns:\n",
        "    if 'total_lines' in X_eng.columns:\n",
        "        X_eng['time_per_line_calc'] = X_eng['execution_time'] / (X_eng['total_lines'] + 1)\n",
        "    if 'cyclomatic_complexity' in X_eng.columns:\n",
        "        X_eng['time_per_complexity'] = X_eng['execution_time'] / (X_eng['cyclomatic_complexity'] + 1)\n",
        "    if 'for_loops' in X_eng.columns:\n",
        "        X_eng['time_per_loop'] = X_eng['execution_time'] / (X_eng['for_loops'] + 1)\n",
        "\n",
        "    # Time efficiency ratios\n",
        "    X_eng['time_squared'] = X_eng['execution_time'] ** 2\n",
        "    X_eng['time_log'] = np.log1p(X_eng['execution_time'])\n",
        "    X_eng['time_sqrt'] = np.sqrt(X_eng['execution_time'])\n",
        "\n",
        "# Memory interactions\n",
        "if 'memory_peak_mb' in X_eng.columns:\n",
        "    if 'total_lines' in X_eng.columns:\n",
        "        X_eng['memory_per_line'] = X_eng['memory_peak_mb'] / (X_eng['total_lines'] + 1)\n",
        "    if 'unique_variables' in X_eng.columns:\n",
        "        X_eng['memory_per_var'] = X_eng['memory_peak_mb'] / (X_eng['unique_variables'] + 1)\n",
        "    if 'classes_count' in X_eng.columns:\n",
        "        X_eng['memory_per_class'] = X_eng['memory_peak_mb'] / (X_eng['classes_count'] + 1)\n",
        "\n",
        "    X_eng['memory_squared'] = X_eng['memory_peak_mb'] ** 2\n",
        "    X_eng['memory_log'] = np.log1p(X_eng['memory_peak_mb'])\n",
        "\n",
        "# CPU interactions\n",
        "if 'cpu_percent_avg' in X_eng.columns:\n",
        "    if 'execution_time' in X_eng.columns:\n",
        "        X_eng['cpu_time_product'] = X_eng['cpu_percent_avg'] * X_eng['execution_time']\n",
        "    if 'cyclomatic_complexity' in X_eng.columns:\n",
        "        X_eng['cpu_complexity_ratio'] = X_eng['cpu_percent_avg'] / (X_eng['cyclomatic_complexity'] + 1)\n",
        "\n",
        "if 'cpu_percent_max' in X_eng.columns and 'cpu_percent_avg' in X_eng.columns:\n",
        "    X_eng['cpu_variability'] = X_eng['cpu_percent_max'] - X_eng['cpu_percent_avg']\n",
        "\n",
        "# === COMBINED ENERGY PROXY ===\n",
        "print(\"   â€¢ Combined energy proxy...\")\n",
        "\n",
        "energy_proxy = 0\n",
        "if 'execution_time' in X_eng.columns:\n",
        "    energy_proxy += X_eng['execution_time'] * 50  # Time is major factor\n",
        "\n",
        "if 'memory_peak_mb' in X_eng.columns:\n",
        "    energy_proxy += X_eng['memory_peak_mb'] * 0.5  # Memory contributes\n",
        "\n",
        "if 'cpu_percent_avg' in X_eng.columns:\n",
        "    energy_proxy += X_eng['cpu_percent_avg'] * 2  # CPU contributes\n",
        "\n",
        "if isinstance(energy_proxy, pd.Series):\n",
        "    X_eng['combined_energy_proxy'] = energy_proxy\n",
        "\n",
        "# === STATIC FEATURE ENGINEERING ===\n",
        "print(\"   â€¢ Static feature polynomials...\")\n",
        "\n",
        "for col in ['total_lines', 'code_lines', 'cyclomatic_complexity', 'total_ast_nodes']:\n",
        "    if col in X_eng.columns:\n",
        "        X_eng[f'{col}_sqrt'] = np.sqrt(X_eng[col] + 1)\n",
        "        X_eng[f'{col}_log'] = np.log1p(X_eng[col])\n",
        "        X_eng[f'{col}_square'] = X_eng[col] ** 2\n",
        "\n",
        "# Static interactions\n",
        "if 'total_lines' in X_eng.columns and 'cyclomatic_complexity' in X_eng.columns:\n",
        "    X_eng['lines_x_complexity'] = X_eng['total_lines'] * X_eng['cyclomatic_complexity']\n",
        "\n",
        "if 'code_lines' in X_eng.columns and 'total_ast_nodes' in X_eng.columns:\n",
        "    X_eng['code_x_ast'] = X_eng['code_lines'] * X_eng['total_ast_nodes']\n",
        "\n",
        "# Ratios\n",
        "if 'code_lines' in X_eng.columns and 'total_lines' in X_eng.columns:\n",
        "    X_eng['code_ratio'] = X_eng['code_lines'] / (X_eng['total_lines'] + 1)\n",
        "\n",
        "if 'comment_lines' in X_eng.columns and 'code_lines' in X_eng.columns:\n",
        "    X_eng['comment_ratio_calc'] = X_eng['comment_lines'] / (X_eng['code_lines'] + 1)\n",
        "\n",
        "print(f\"âœ… Engineered features: {X_eng.shape[1]} total\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: LOG TRANSFORM TARGET\n",
        "# ============================================================================\n",
        "y_log = np.log1p(y)\n",
        "print(f\"\\nðŸ“ˆ Target (log): [{y_log.min():.2f}, {y_log.max():.2f}]\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: FEATURE SELECTION\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸŽ¯ Feature Selection...\")\n",
        "\n",
        "k_best = min(80, X_eng.shape[1])\n",
        "selector = SelectKBest(score_func=mutual_info_regression, k=k_best)\n",
        "selector.fit(X_eng, y_log)\n",
        "X_selected = X_eng.loc[:, selector.get_support()]\n",
        "\n",
        "print(f\"âœ… Selected {X_selected.shape[1]} features\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: NORMALIZE\n",
        "# ============================================================================\n",
        "qt = QuantileTransformer(\n",
        "    n_quantiles=min(1000, X_selected.shape[0]),\n",
        "    output_distribution='normal',\n",
        "    random_state=42\n",
        ")\n",
        "X_transformed = pd.DataFrame(\n",
        "    qt.fit_transform(X_selected),\n",
        "    columns=X_selected.columns,\n",
        "    index=X_selected.index\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 8: STRATIFIED SPLIT\n",
        "# ============================================================================\n",
        "y_bins = pd.qcut(y_log, q=5, labels=False, duplicates='drop')\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_transformed, y_log, test_size=0.15, random_state=42,\n",
        "    stratify=y_bins, shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"\\nðŸ“Š Split: {X_train.shape[0]:,} train, {X_test.shape[0]:,} test\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 9: OPTIMIZED STACKING ENSEMBLE\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸš€ Training Stacking Ensemble (Static + Dynamic Features)...\")\n",
        "\n",
        "# With dynamic features, we can use less regularization\n",
        "base_models = [\n",
        "    ('lgbm1', LGBMRegressor(\n",
        "        n_estimators=1500, learning_rate=0.01, num_leaves=50,\n",
        "        max_depth=10, min_child_samples=10, subsample=0.9,\n",
        "        colsample_bytree=0.9, reg_alpha=0.1, reg_lambda=0.1,\n",
        "        random_state=42, verbose=-1, n_jobs=-1\n",
        "    )),\n",
        "    ('lgbm2', LGBMRegressor(\n",
        "        n_estimators=1200, learning_rate=0.015, num_leaves=35,\n",
        "        max_depth=8, min_child_samples=15, subsample=0.85,\n",
        "        colsample_bytree=0.85, reg_alpha=0.3, reg_lambda=0.3,\n",
        "        random_state=43, verbose=-1, n_jobs=-1\n",
        "    )),\n",
        "    ('xgb1', XGBRegressor(\n",
        "        n_estimators=1500, learning_rate=0.01, max_depth=10,\n",
        "        min_child_weight=1, subsample=0.9, colsample_bytree=0.9,\n",
        "        reg_alpha=0.1, reg_lambda=0.1, random_state=42,\n",
        "        verbosity=0, n_jobs=-1\n",
        "    )),\n",
        "    ('xgb2', XGBRegressor(\n",
        "        n_estimators=1200, learning_rate=0.015, max_depth=8,\n",
        "        min_child_weight=2, subsample=0.85, colsample_bytree=0.85,\n",
        "        reg_alpha=0.3, reg_lambda=0.3, random_state=43,\n",
        "        verbosity=0, n_jobs=-1\n",
        "    )),\n",
        "    ('ridge', Ridge(alpha=1.0)),\n",
        "    ('elastic', ElasticNet(alpha=0.001, l1_ratio=0.5, max_iter=5000))\n",
        "]\n",
        "\n",
        "meta_model = Ridge(alpha=1.0)\n",
        "\n",
        "stacking_model = StackingRegressor(\n",
        "    estimators=base_models,\n",
        "    final_estimator=meta_model,\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"   Training ensemble...\")\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 10: EVALUATE\n",
        "# ============================================================================\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(\"MODEL PERFORMANCE: STATIC + DYNAMIC (NO LEAKAGE)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "y_train_pred_log = stacking_model.predict(X_train)\n",
        "y_test_pred_log = stacking_model.predict(X_test)\n",
        "\n",
        "r2_train_log = r2_score(y_train, y_train_pred_log)\n",
        "r2_test_log = r2_score(y_test, y_test_pred_log)\n",
        "\n",
        "print(f\"\\nðŸ“Š Log-Scale Performance:\")\n",
        "print(f\"   Training RÂ²: {r2_train_log:.6f}\")\n",
        "print(f\"   Test RÂ²:     {r2_test_log:.6f}\")\n",
        "print(f\"   Overfitting Gap: {r2_train_log - r2_test_log:.6f}\")\n",
        "\n",
        "# Original scale\n",
        "y_train_actual = np.expm1(y_train)\n",
        "y_train_pred = np.expm1(y_train_pred_log)\n",
        "y_test_actual = np.expm1(y_test)\n",
        "y_test_pred = np.expm1(y_test_pred_log)\n",
        "\n",
        "r2_train_original = r2_score(y_train_actual, y_train_pred)\n",
        "r2_test_original = r2_score(y_test_actual, y_test_pred)\n",
        "\n",
        "print(f\"\\nâš¡ Original Joules Scale:\")\n",
        "print(f\"   Training RÂ²: {r2_train_original:.6f}\")\n",
        "print(f\"   Test RÂ²:     {r2_test_original:.6f}\")\n",
        "\n",
        "mae_test = mean_absolute_error(y_test_actual, y_test_pred)\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test_actual, y_test_pred))\n",
        "mean_energy = y_test_actual.mean()\n",
        "\n",
        "print(f\"\\nðŸ“ Error Metrics:\")\n",
        "print(f\"   Test MAE:  {mae_test:,.2f} J ({(mae_test/mean_energy)*100:.1f}% of mean)\")\n",
        "print(f\"   Test RMSE: {rmse_test:,.2f} J\")\n",
        "print(f\"   Mean Energy: {mean_energy:,.2f} J\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 11: CROSS-VALIDATION\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸ”„ 5-Fold Cross-Validation:\")\n",
        "cv_scores = cross_val_score(\n",
        "    stacking_model, X_transformed, y_log, cv=5,\n",
        "    scoring='r2', n_jobs=-1\n",
        ")\n",
        "print(f\"   CV RÂ²: {[f'{s:.4f}' for s in cv_scores]}\")\n",
        "print(f\"   Mean: {cv_scores.mean():.6f} (Â±{cv_scores.std():.6f})\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 12: FEATURE IMPORTANCE ANALYSIS\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸŽ¯ Top 20 Features (Static vs Dynamic):\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Get importance from LGBM\n",
        "lgbm_model = base_models[0][1]\n",
        "lgbm_model.fit(X_train, y_train)\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': lgbm_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "for idx, row in feature_importance.head(20).iterrows():\n",
        "    feat = row['feature']\n",
        "    # Mark feature type\n",
        "    if any(d in feat for d in ['execution_time', 'memory', 'cpu']):\n",
        "        marker = \"ðŸ”¥ [DYNAMIC]\"\n",
        "    elif any(s in feat for s in ['time_per', 'memory_per', 'cpu_']):\n",
        "        marker = \"âš¡ [HYBRID]\"\n",
        "    else:\n",
        "        marker = \"ðŸ“Š [STATIC] \"\n",
        "\n",
        "    print(f\"   {marker} {feat:40s} {row['importance']:8.1f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 13: DYNAMIC FEATURE CONTRIBUTION ANALYSIS\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸ”¬ Dynamic Feature Impact Analysis:\")\n",
        "\n",
        "# Count dynamic features in top 20\n",
        "top_20_features = feature_importance.head(20)['feature'].tolist()\n",
        "dynamic_in_top = sum(1 for f in top_20_features\n",
        "                     if any(d in f for d in ['execution_time', 'memory', 'cpu']))\n",
        "hybrid_in_top = sum(1 for f in top_20_features\n",
        "                    if any(h in f for h in ['time_per', 'memory_per', 'cpu_']))\n",
        "\n",
        "print(f\"   Dynamic features in Top 20: {dynamic_in_top}\")\n",
        "print(f\"   Hybrid features in Top 20: {hybrid_in_top}\")\n",
        "print(f\"   Static features in Top 20: {20 - dynamic_in_top - hybrid_in_top}\")\n",
        "\n",
        "# Total importance by type\n",
        "total_importance = feature_importance['importance'].sum()\n",
        "dynamic_importance = feature_importance[\n",
        "    feature_importance['feature'].str.contains('execution_time|memory_|cpu_', regex=True)\n",
        "]['importance'].sum()\n",
        "\n",
        "print(f\"\\n   Dynamic feature importance: {dynamic_importance:,.0f} ({dynamic_importance/total_importance*100:.1f}%)\")\n",
        "print(f\"   Static feature importance: {total_importance - dynamic_importance:,.0f} ({(1-dynamic_importance/total_importance)*100:.1f}%)\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 14: COMPARE WITH STATIC-ONLY\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸ“Š Performance Comparison:\")\n",
        "print(f\"   Static-only model (previous): RÂ² ~ 0.53-0.65\")\n",
        "print(f\"   Static + Dynamic (current):   RÂ² = {r2_test_log:.4f}\")\n",
        "print(f\"   Improvement: +{(r2_test_log - 0.59) * 100:.1f} percentage points\")\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL SUMMARY\n",
        "# ============================================================================\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(\"ðŸŽ‰ FINAL RESULTS - STATIC + DYNAMIC FEATURES\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"âœ… Samples: {X_train.shape[0]:,} train, {X_test.shape[0]:,} test\")\n",
        "print(f\"âœ… Features: {X_transformed.shape[1]} (static + dynamic, zero leakage)\")\n",
        "print(f\"âœ… Dynamic features used: {len(dynamic_used)}\")\n",
        "print(f\"âœ… Test RÂ² (Log): {r2_test_log:.6f}\")\n",
        "print(f\"âœ… Test RÂ² (Original): {r2_test_original:.6f}\")\n",
        "print(f\"âœ… CV RÂ²: {cv_scores.mean():.6f} Â±{cv_scores.std():.4f}\")\n",
        "print(f\"âœ… MAE: {mae_test:,.2f} J ({(mae_test/mean_energy)*100:.1f}%)\")\n",
        "print(f\"âœ… Overfitting: {r2_train_log - r2_test_log:.4f}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nðŸ”’ Leakage Prevention Summary:\")\n",
        "print(f\"   âœ… Excluded: {len(LEAKAGE_FEATURES)} leakage features\")\n",
        "print(f\"   âœ… Included: {len(dynamic_used)} clean dynamic features\")\n",
        "print(f\"   âœ… No energy-derived features used\")\n",
        "\n",
        "if r2_test_log >= 0.9:\n",
        "    print(\"\\nðŸŽŠ EXCELLENT: RÂ² â‰¥ 0.90 - Near-perfect prediction!\")\n",
        "elif r2_test_log >= 0.8:\n",
        "    print(\"\\nðŸš€ OUTSTANDING: RÂ² â‰¥ 0.80 - Production-ready!\")\n",
        "elif r2_test_log >= 0.7:\n",
        "    print(\"\\nâœ… STRONG: RÂ² â‰¥ 0.70 - Very good performance!\")\n",
        "else:\n",
        "    print(f\"\\nðŸ“Š Current: RÂ² = {r2_test_log:.3f}\")\n",
        "\n",
        "print(f\"\\nðŸ’¡ Key Insights:\")\n",
        "print(f\"   â€¢ Dynamic features explain {dynamic_importance/total_importance*100:.0f}% of prediction power\")\n",
        "print(f\"   â€¢ execution_time likely most important predictor\")\n",
        "print(f\"   â€¢ Model shows upper bound of predictability\")\n",
        "print(f\"   â€¢ Can be used for post-execution profiling/optimization\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "kPKNfEb9LKDY",
        "outputId": "0808ed05-f57e-460e-b606-5694b0affce8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STATIC + DYNAMIC FEATURES MODEL (ZERO LEAKAGE)\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š Initial Dataset: 6,711 rows Ã— 107 columns\n",
            "âœ… Valid samples: 1,567 rows\n",
            "\n",
            "ðŸ”’ Building feature set with STRICT leakage prevention...\n",
            "\n",
            "ðŸ“‹ Feature Breakdown:\n",
            "   Static features: 86\n",
            "   Dynamic features: 5\n",
            "   Gray features: 7\n",
            "   âœ… Total usable: 98\n",
            "   âœ… No leakage detected!\n",
            "\n",
            "ðŸ”¥ Dynamic features included:\n",
            "      â€¢ execution_time\n",
            "      â€¢ cpu_percent_avg\n",
            "      â€¢ cpu_percent_max\n",
            "      â€¢ memory_usage_mb\n",
            "      â€¢ memory_peak_mb\n",
            "\n",
            "ðŸ“ Feature Matrix: 1,567 rows Ã— 98 features\n",
            "\n",
            "ðŸ§¹ Data cleaning...\n",
            "   Removed 6 constant features\n",
            "âœ… Clean data: 1,551 samples, 92 features\n",
            "\n",
            "ðŸ”§ Feature Engineering...\n",
            "   â€¢ Static Ã— Dynamic interactions...\n",
            "   â€¢ Combined energy proxy...\n",
            "   â€¢ Static feature polynomials...\n",
            "âœ… Engineered features: 123 total\n",
            "\n",
            "ðŸ“ˆ Target (log): [1.46, 6.18]\n",
            "\n",
            "ðŸŽ¯ Feature Selection...\n",
            "âœ… Selected 80 features\n",
            "\n",
            "ðŸ“Š Split: 1,318 train, 233 test\n",
            "\n",
            "ðŸš€ Training Stacking Ensemble (Static + Dynamic Features)...\n",
            "   Training ensemble...\n",
            "\n",
            "================================================================================\n",
            "MODEL PERFORMANCE: STATIC + DYNAMIC (NO LEAKAGE)\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š Log-Scale Performance:\n",
            "   Training RÂ²: 0.999988\n",
            "   Test RÂ²:     0.999962\n",
            "   Overfitting Gap: 0.000026\n",
            "\n",
            "âš¡ Original Joules Scale:\n",
            "   Training RÂ²: 0.999897\n",
            "   Test RÂ²:     0.999743\n",
            "\n",
            "ðŸ“ Error Metrics:\n",
            "   Test MAE:  0.47 J (0.9% of mean)\n",
            "   Test RMSE: 1.35 J\n",
            "   Mean Energy: 54.32 J\n",
            "\n",
            "ðŸ”„ 5-Fold Cross-Validation:\n",
            "   CV RÂ²: ['1.0000', '1.0000', '1.0000', '1.0000', '1.0000']\n",
            "   Mean: 0.999962 (Â±0.000004)\n",
            "\n",
            "ðŸŽ¯ Top 20 Features (Static vs Dynamic):\n",
            "--------------------------------------------------------------------------------\n",
            "   ðŸ”¥ [DYNAMIC] execution_time                             7399.0\n",
            "   ðŸ”¥ [DYNAMIC] cpu_time_product                           1515.0\n",
            "   âš¡ [HYBRID] time_per_loop                              1175.0\n",
            "   ðŸ“Š [STATIC]  std_line_length                            1046.0\n",
            "   ðŸ”¥ [DYNAMIC] cpu_variability                            1009.0\n",
            "   ðŸ“Š [STATIC]  combined_energy_proxy                      1001.0\n",
            "   âš¡ [HYBRID] time_per_complexity                         851.0\n",
            "   ðŸ“Š [STATIC]  function_ratio                              839.0\n",
            "   ðŸ“Š [STATIC]  complexity_per_line                         831.0\n",
            "   ðŸ“Š [STATIC]  time_squared                                766.0\n",
            "   ðŸ“Š [STATIC]  single_quotes                               765.0\n",
            "   ðŸ“Š [STATIC]  total_quotes                                757.0\n",
            "   ðŸ”¥ [DYNAMIC] memory_usage_mb                             687.0\n",
            "   âš¡ [HYBRID] time_per_line_calc                          647.0\n",
            "   ðŸ“Š [STATIC]  comment_ratio                               625.0\n",
            "   ðŸ“Š [STATIC]  control_flow_ratio                          621.0\n",
            "   ðŸ”¥ [DYNAMIC] cpu_complexity_ratio                        552.0\n",
            "   ðŸ“Š [STATIC]  unary_operations                            550.0\n",
            "   ðŸ“Š [STATIC]  code_ratio                                  537.0\n",
            "   ðŸ“Š [STATIC]  double_quotes                               531.0\n",
            "\n",
            "ðŸ”¬ Dynamic Feature Impact Analysis:\n",
            "   Dynamic features in Top 20: 5\n",
            "   Hybrid features in Top 20: 6\n",
            "   Static features in Top 20: 9\n",
            "\n",
            "   Dynamic feature importance: 13,155 (40.3%)\n",
            "   Static feature importance: 19,507 (59.7%)\n",
            "\n",
            "ðŸ“Š Performance Comparison:\n",
            "   Static-only model (previous): RÂ² ~ 0.53-0.65\n",
            "   Static + Dynamic (current):   RÂ² = 1.0000\n",
            "   Improvement: +41.0 percentage points\n",
            "\n",
            "================================================================================\n",
            "ðŸŽ‰ FINAL RESULTS - STATIC + DYNAMIC FEATURES\n",
            "================================================================================\n",
            "âœ… Samples: 1,318 train, 233 test\n",
            "âœ… Features: 80 (static + dynamic, zero leakage)\n",
            "âœ… Dynamic features used: 5\n",
            "âœ… Test RÂ² (Log): 0.999962\n",
            "âœ… Test RÂ² (Original): 0.999743\n",
            "âœ… CV RÂ²: 0.999962 Â±0.0000\n",
            "âœ… MAE: 0.47 J (0.9%)\n",
            "âœ… Overfitting: 0.0000\n",
            "================================================================================\n",
            "\n",
            "ðŸ”’ Leakage Prevention Summary:\n",
            "   âœ… Excluded: 7 leakage features\n",
            "   âœ… Included: 5 clean dynamic features\n",
            "   âœ… No energy-derived features used\n",
            "\n",
            "ðŸŽŠ EXCELLENT: RÂ² â‰¥ 0.90 - Near-perfect prediction!\n",
            "\n",
            "ðŸ’¡ Key Insights:\n",
            "   â€¢ Dynamic features explain 40% of prediction power\n",
            "   â€¢ execution_time likely most important predictor\n",
            "   â€¢ Model shows upper bound of predictability\n",
            "   â€¢ Can be used for post-execution profiling/optimization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.linear_model import Ridge, ElasticNet\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "DATASET_PATH = '/content/Py_Raw.csv'\n",
        "\n",
        "# ============================================================================\n",
        "# FEATURE CATEGORIZATION - STRICT LEAKAGE PREVENTION\n",
        "# ============================================================================\n",
        "\n",
        "# TARGET (what we're predicting)\n",
        "TARGET = 'energy_consumption_joules'\n",
        "\n",
        "# LEAKAGE FEATURES - MUST BE REMOVED\n",
        "# These are derived FROM the target or highly correlated outcomes\n",
        "LEAKAGE_FEATURES = [\n",
        "    'co2_emissions_g',           # Direct calculation from energy\n",
        "    'energy_per_line',           # Derived: energy / lines\n",
        "    'time_per_line',             # Derived: time / lines\n",
        "    'co2_per_line',              # Derived: co2 / lines\n",
        "    'energy_pkg_joules',         # Alternative energy measurement\n",
        "    'energy_ram_joules',         # Alternative energy measurement\n",
        "    'power_draw_watts',          # Direct energy measurement\n",
        "]\n",
        "\n",
        "# DYNAMIC FEATURES - CAN USE (measured during execution, but not derived from energy)\n",
        "DYNAMIC_FEATURES = [\n",
        "    'execution_time',            # Independent measurement\n",
        "    'cpu_percent_avg',           # Independent measurement\n",
        "    'cpu_percent_max',           # Independent measurement\n",
        "    'memory_usage_mb',           # Independent measurement\n",
        "    'memory_peak_mb',            # Independent measurement\n",
        "]\n",
        "\n",
        "# GRAY FEATURES - CAN USE\n",
        "GRAY_FEATURES = [\n",
        "    'executed_successfully', 'execution_error', 'exit_code',\n",
        "    'syntax_valid', 'parse_error', 'syntax_error_line', 'syntax_error_type'\n",
        "]\n",
        "\n",
        "# STATIC FEATURES - CAN USE\n",
        "STATIC_FEATURES = [\n",
        "    'total_lines', 'code_lines', 'blank_lines', 'total_characters',\n",
        "    'file_size_bytes', 'file_size_kb', 'avg_line_length', 'max_line_length',\n",
        "    'min_line_length', 'std_line_length', 'comment_lines', 'single_quotes',\n",
        "    'double_quotes', 'triple_quotes', 'total_quotes', 'parentheses_count',\n",
        "    'brackets_count', 'braces_count', 'semicolon_count', 'colon_count',\n",
        "    'unique_indentation_levels', 'avg_indentation', 'max_indentation',\n",
        "    'std_indentation', 'keyword_def_count', 'keyword_class_count',\n",
        "    'keyword_import_count', 'keyword_from_count', 'keyword_if_count',\n",
        "    'keyword_else_count', 'keyword_elif_count', 'keyword_for_count',\n",
        "    'keyword_while_count', 'keyword_try_count', 'keyword_except_count',\n",
        "    'keyword_finally_count', 'keyword_with_count', 'keyword_return_count',\n",
        "    'keyword_yield_count', 'keyword_lambda_count', 'keyword_async_count',\n",
        "    'keyword_await_count', 'keyword_global_count', 'keyword_nonlocal_count',\n",
        "    'functions_count', 'async_functions_count', 'classes_count',\n",
        "    'imports_count', 'if_statements', 'for_loops', 'while_loops',\n",
        "    'async_for_loops', 'try_blocks', 'with_statements', 'async_with_statements',\n",
        "    'lambda_functions', 'list_comprehensions', 'dict_comprehensions',\n",
        "    'set_comprehensions', 'generator_expressions', 'binary_operations',\n",
        "    'unary_operations', 'boolean_operations', 'comparisons', 'decorators',\n",
        "    'assignments', 'augmented_assignments', 'return_statements',\n",
        "    'yield_statements', 'raise_statements', 'assert_statements',\n",
        "    'cyclomatic_complexity', 'max_nesting_depth', 'unique_function_names',\n",
        "    'unique_class_names', 'unique_imports', 'unique_variables',\n",
        "    'total_ast_nodes', 'comment_ratio', 'control_flow_ratio',\n",
        "    'function_ratio', 'class_ratio', 'code_density', 'complexity_per_line',\n",
        "    'functions_per_line', 'avg_complexity_per_function'\n",
        "]\n",
        "\n",
        "# METADATA - EXCLUDE\n",
        "METADATA_COLS = [\n",
        "    'file_path', 'file_name', 'repo_name', 'repo_url', 'timestamp',\n",
        "    'run_id', 'index', 'id', 'file_id', 'repository', 'author',\n",
        "    'date_created', 'date_modified', 'file_index'\n",
        "]\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: LOAD DATA\n",
        "# ============================================================================\n",
        "print(\"=\" * 80)\n",
        "print(\"STATIC + DYNAMIC FEATURES MODEL (ZERO LEAKAGE)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "print(f\"\\nðŸ“Š Initial Dataset: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
        "\n",
        "if 'executed_successfully' in df.columns:\n",
        "    df = df[df['executed_successfully'] == True].copy()\n",
        "\n",
        "df = df[df[TARGET] > 0].copy()\n",
        "df = df.dropna(subset=[TARGET])\n",
        "print(f\"âœ… Valid samples: {df.shape[0]:,} rows\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: BUILD FEATURE SET (PREVENT LEAKAGE!)\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸ”’ Building feature set with STRICT leakage prevention...\")\n",
        "\n",
        "available_cols = df.columns.tolist()\n",
        "\n",
        "# Collect all usable features\n",
        "usable_features = []\n",
        "for col in available_cols:\n",
        "    # Include if it's static, dynamic, or gray\n",
        "    if col in STATIC_FEATURES or col in DYNAMIC_FEATURES or col in GRAY_FEATURES:\n",
        "        # Exclude if it's target, metadata, or leakage\n",
        "        if col not in [TARGET] + METADATA_COLS + LEAKAGE_FEATURES:\n",
        "            usable_features.append(col)\n",
        "\n",
        "# Categorize features\n",
        "static_used = [f for f in usable_features if f in STATIC_FEATURES]\n",
        "dynamic_used = [f for f in usable_features if f in DYNAMIC_FEATURES]\n",
        "gray_used = [f for f in usable_features if f in GRAY_FEATURES]\n",
        "\n",
        "print(f\"\\nðŸ“‹ Feature Breakdown:\")\n",
        "print(f\"   Static features: {len(static_used)}\")\n",
        "print(f\"   Dynamic features: {len(dynamic_used)}\")\n",
        "print(f\"   Gray features: {len(gray_used)}\")\n",
        "print(f\"   âœ… Total usable: {len(usable_features)}\")\n",
        "\n",
        "# Verify no leakage\n",
        "leaked = [f for f in usable_features if f in LEAKAGE_FEATURES]\n",
        "if leaked:\n",
        "    print(f\"   âŒ ERROR: Leaked features found: {leaked}\")\n",
        "    exit()\n",
        "else:\n",
        "    print(f\"   âœ… No leakage detected!\")\n",
        "\n",
        "# Show dynamic features being used\n",
        "if dynamic_used:\n",
        "    print(f\"\\nðŸ”¥ Dynamic features included:\")\n",
        "    for feat in dynamic_used:\n",
        "        print(f\"      â€¢ {feat}\")\n",
        "\n",
        "X = df[usable_features].copy()\n",
        "y = df[TARGET].copy()\n",
        "\n",
        "print(f\"\\nðŸ“ Feature Matrix: {X.shape[0]:,} rows Ã— {X.shape[1]} features\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: MINIMAL CLEANING\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸ§¹ Data cleaning...\")\n",
        "\n",
        "# Handle infinite/NaN\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "for col in X.columns:\n",
        "    if X[col].dtype in ['float64', 'int64']:\n",
        "        if X[col].isna().sum() > 0:\n",
        "            X[col].fillna(X[col].median(), inplace=True)\n",
        "\n",
        "# Remove constant features\n",
        "constant_cols = [col for col in X.columns if X[col].nunique() <= 1]\n",
        "if constant_cols:\n",
        "    X = X.drop(columns=constant_cols)\n",
        "    print(f\"   Removed {len(constant_cols)} constant features\")\n",
        "\n",
        "# Very minimal outlier removal\n",
        "q_low = y.quantile(0.005)\n",
        "q_high = y.quantile(0.995)\n",
        "mask = (y >= q_low) & (y <= q_high)\n",
        "X = X[mask]\n",
        "y = y[mask]\n",
        "\n",
        "print(f\"âœ… Clean data: {X.shape[0]:,} samples, {X.shape[1]} features\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: FEATURE ENGINEERING\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸ”§ Feature Engineering...\")\n",
        "\n",
        "X_eng = X.copy()\n",
        "\n",
        "# === INTERACTIONS BETWEEN STATIC AND DYNAMIC ===\n",
        "print(\"   â€¢ Static Ã— Dynamic interactions...\")\n",
        "\n",
        "# Execution time interactions\n",
        "if 'execution_time' in X_eng.columns:\n",
        "    if 'total_lines' in X_eng.columns:\n",
        "        X_eng['time_per_line_calc'] = X_eng['execution_time'] / (X_eng['total_lines'] + 1)\n",
        "    if 'cyclomatic_complexity' in X_eng.columns:\n",
        "        X_eng['time_per_complexity'] = X_eng['execution_time'] / (X_eng['cyclomatic_complexity'] + 1)\n",
        "    if 'for_loops' in X_eng.columns:\n",
        "        X_eng['time_per_loop'] = X_eng['execution_time'] / (X_eng['for_loops'] + 1)\n",
        "\n",
        "    # Time efficiency ratios\n",
        "    X_eng['time_squared'] = X_eng['execution_time'] ** 2\n",
        "    X_eng['time_log'] = np.log1p(X_eng['execution_time'])\n",
        "    X_eng['time_sqrt'] = np.sqrt(X_eng['execution_time'])\n",
        "\n",
        "# Memory interactions\n",
        "if 'memory_peak_mb' in X_eng.columns:\n",
        "    if 'total_lines' in X_eng.columns:\n",
        "        X_eng['memory_per_line'] = X_eng['memory_peak_mb'] / (X_eng['total_lines'] + 1)\n",
        "    if 'unique_variables' in X_eng.columns:\n",
        "        X_eng['memory_per_var'] = X_eng['memory_peak_mb'] / (X_eng['unique_variables'] + 1)\n",
        "    if 'classes_count' in X_eng.columns:\n",
        "        X_eng['memory_per_class'] = X_eng['memory_peak_mb'] / (X_eng['classes_count'] + 1)\n",
        "\n",
        "    X_eng['memory_squared'] = X_eng['memory_peak_mb'] ** 2\n",
        "    X_eng['memory_log'] = np.log1p(X_eng['memory_peak_mb'])\n",
        "\n",
        "# CPU interactions\n",
        "if 'cpu_percent_avg' in X_eng.columns:\n",
        "    if 'execution_time' in X_eng.columns:\n",
        "        X_eng['cpu_time_product'] = X_eng['cpu_percent_avg'] * X_eng['execution_time']\n",
        "    if 'cyclomatic_complexity' in X_eng.columns:\n",
        "        X_eng['cpu_complexity_ratio'] = X_eng['cpu_percent_avg'] / (X_eng['cyclomatic_complexity'] + 1)\n",
        "\n",
        "if 'cpu_percent_max' in X_eng.columns and 'cpu_percent_avg' in X_eng.columns:\n",
        "    X_eng['cpu_variability'] = X_eng['cpu_percent_max'] - X_eng['cpu_percent_avg']\n",
        "\n",
        "# === COMBINED ENERGY PROXY ===\n",
        "print(\"   â€¢ Combined energy proxy...\")\n",
        "\n",
        "energy_proxy = 0\n",
        "if 'execution_time' in X_eng.columns:\n",
        "    energy_proxy += X_eng['execution_time'] * 50  # Time is major factor\n",
        "\n",
        "if 'memory_peak_mb' in X_eng.columns:\n",
        "    energy_proxy += X_eng['memory_peak_mb'] * 0.5  # Memory contributes\n",
        "\n",
        "if 'cpu_percent_avg' in X_eng.columns:\n",
        "    energy_proxy += X_eng['cpu_percent_avg'] * 2  # CPU contributes\n",
        "\n",
        "if isinstance(energy_proxy, pd.Series):\n",
        "    X_eng['combined_energy_proxy'] = energy_proxy\n",
        "\n",
        "# === STATIC FEATURE ENGINEERING ===\n",
        "print(\"   â€¢ Static feature polynomials...\")\n",
        "\n",
        "for col in ['total_lines', 'code_lines', 'cyclomatic_complexity', 'total_ast_nodes']:\n",
        "    if col in X_eng.columns:\n",
        "        X_eng[f'{col}_sqrt'] = np.sqrt(X_eng[col] + 1)\n",
        "        X_eng[f'{col}_log'] = np.log1p(X_eng[col])\n",
        "        X_eng[f'{col}_square'] = X_eng[col] ** 2\n",
        "\n",
        "# Static interactions\n",
        "if 'total_lines' in X_eng.columns and 'cyclomatic_complexity' in X_eng.columns:\n",
        "    X_eng['lines_x_complexity'] = X_eng['total_lines'] * X_eng['cyclomatic_complexity']\n",
        "\n",
        "if 'code_lines' in X_eng.columns and 'total_ast_nodes' in X_eng.columns:\n",
        "    X_eng['code_x_ast'] = X_eng['code_lines'] * X_eng['total_ast_nodes']\n",
        "\n",
        "# Ratios\n",
        "if 'code_lines' in X_eng.columns and 'total_lines' in X_eng.columns:\n",
        "    X_eng['code_ratio'] = X_eng['code_lines'] / (X_eng['total_lines'] + 1)\n",
        "\n",
        "if 'comment_lines' in X_eng.columns and 'code_lines' in X_eng.columns:\n",
        "    X_eng['comment_ratio_calc'] = X_eng['comment_lines'] / (X_eng['code_lines'] + 1)\n",
        "\n",
        "print(f\"âœ… Engineered features: {X_eng.shape[1]} total\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: LOG TRANSFORM TARGET\n",
        "# ============================================================================\n",
        "y_log = np.log1p(y)\n",
        "print(f\"\\nðŸ“ˆ Target (log): [{y_log.min():.2f}, {y_log.max():.2f}]\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: FEATURE SELECTION\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸŽ¯ Feature Selection...\")\n",
        "\n",
        "k_best = min(80, X_eng.shape[1])\n",
        "selector = SelectKBest(score_func=mutual_info_regression, k=k_best)\n",
        "selector.fit(X_eng, y_log)\n",
        "X_selected = X_eng.loc[:, selector.get_support()]\n",
        "\n",
        "print(f\"âœ… Selected {X_selected.shape[1]} features\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: NORMALIZE\n",
        "# ============================================================================\n",
        "qt = QuantileTransformer(\n",
        "    n_quantiles=min(1000, X_selected.shape[0]),\n",
        "    output_distribution='normal',\n",
        "    random_state=42\n",
        ")\n",
        "X_transformed = pd.DataFrame(\n",
        "    qt.fit_transform(X_selected),\n",
        "    columns=X_selected.columns,\n",
        "    index=X_selected.index\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 8: STRATIFIED SPLIT\n",
        "# ============================================================================\n",
        "y_bins = pd.qcut(y_log, q=5, labels=False, duplicates='drop')\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_transformed, y_log, test_size=0.15, random_state=42,\n",
        "    stratify=y_bins, shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"\\nðŸ“Š Split: {X_train.shape[0]:,} train, {X_test.shape[0]:,} test\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 9: OPTIMIZED STACKING ENSEMBLE\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸš€ Training Stacking Ensemble (Static + Dynamic Features)...\")\n",
        "\n",
        "# With dynamic features, we can use less regularization\n",
        "base_models = [\n",
        "    ('lgbm1', LGBMRegressor(\n",
        "        n_estimators=1500, learning_rate=0.01, num_leaves=50,\n",
        "        max_depth=10, min_child_samples=10, subsample=0.9,\n",
        "        colsample_bytree=0.9, reg_alpha=0.1, reg_lambda=0.1,\n",
        "        random_state=42, verbose=-1, n_jobs=-1\n",
        "    )),\n",
        "    ('lgbm2', LGBMRegressor(\n",
        "        n_estimators=1200, learning_rate=0.015, num_leaves=35,\n",
        "        max_depth=8, min_child_samples=15, subsample=0.85,\n",
        "        colsample_bytree=0.85, reg_alpha=0.3, reg_lambda=0.3,\n",
        "        random_state=43, verbose=-1, n_jobs=-1\n",
        "    )),\n",
        "    ('xgb1', XGBRegressor(\n",
        "        n_estimators=1500, learning_rate=0.01, max_depth=10,\n",
        "        min_child_weight=1, subsample=0.9, colsample_bytree=0.9,\n",
        "        reg_alpha=0.1, reg_lambda=0.1, random_state=42,\n",
        "        verbosity=0, n_jobs=-1\n",
        "    )),\n",
        "    ('xgb2', XGBRegressor(\n",
        "        n_estimators=1200, learning_rate=0.015, max_depth=8,\n",
        "        min_child_weight=2, subsample=0.85, colsample_bytree=0.85,\n",
        "        reg_alpha=0.3, reg_lambda=0.3, random_state=43,\n",
        "        verbosity=0, n_jobs=-1\n",
        "    )),\n",
        "    ('ridge', Ridge(alpha=1.0)),\n",
        "    ('elastic', ElasticNet(alpha=0.001, l1_ratio=0.5, max_iter=5000))\n",
        "]\n",
        "\n",
        "meta_model = Ridge(alpha=1.0)\n",
        "\n",
        "stacking_model = StackingRegressor(\n",
        "    estimators=base_models,\n",
        "    final_estimator=meta_model,\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"   Training ensemble...\")\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 10: EVALUATE\n",
        "# ============================================================================\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(\"MODEL PERFORMANCE: STATIC + DYNAMIC (NO LEAKAGE)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "y_train_pred_log = stacking_model.predict(X_train)\n",
        "y_test_pred_log = stacking_model.predict(X_test)\n",
        "\n",
        "r2_train_log = r2_score(y_train, y_train_pred_log)\n",
        "r2_test_log = r2_score(y_test, y_test_pred_log)\n",
        "\n",
        "print(f\"\\nðŸ“Š Log-Scale Performance:\")\n",
        "print(f\"   Training RÂ²: {r2_train_log:.6f}\")\n",
        "print(f\"   Test RÂ²:     {r2_test_log:.6f}\")\n",
        "print(f\"   Overfitting Gap: {r2_train_log - r2_test_log:.6f}\")\n",
        "\n",
        "# Original scale\n",
        "y_train_actual = np.expm1(y_train)\n",
        "y_train_pred = np.expm1(y_train_pred_log)\n",
        "y_test_actual = np.expm1(y_test)\n",
        "y_test_pred = np.expm1(y_test_pred_log)\n",
        "\n",
        "r2_train_original = r2_score(y_train_actual, y_train_pred)\n",
        "r2_test_original = r2_score(y_test_actual, y_test_pred)\n",
        "\n",
        "print(f\"\\nâš¡ Original Joules Scale:\")\n",
        "print(f\"   Training RÂ²: {r2_train_original:.6f}\")\n",
        "print(f\"   Test RÂ²:     {r2_test_original:.6f}\")\n",
        "\n",
        "mae_test = mean_absolute_error(y_test_actual, y_test_pred)\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test_actual, y_test_pred))\n",
        "mean_energy = y_test_actual.mean()\n",
        "\n",
        "print(f\"\\nðŸ“ Error Metrics:\")\n",
        "print(f\"   Test MAE:  {mae_test:,.2f} J ({(mae_test/mean_energy)*100:.1f}% of mean)\")\n",
        "print(f\"   Test RMSE: {rmse_test:,.2f} J\")\n",
        "print(f\"   Mean Energy: {mean_energy:,.2f} J\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 11: CROSS-VALIDATION\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸ”„ 5-Fold Cross-Validation:\")\n",
        "cv_scores = cross_val_score(\n",
        "    stacking_model, X_transformed, y_log, cv=5,\n",
        "    scoring='r2', n_jobs=-1\n",
        ")\n",
        "print(f\"   CV RÂ²: {[f'{s:.4f}' for s in cv_scores]}\")\n",
        "print(f\"   Mean: {cv_scores.mean():.6f} (Â±{cv_scores.std():.6f})\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 12: FEATURE IMPORTANCE ANALYSIS\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸŽ¯ Top 20 Features (Static vs Dynamic):\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Get importance from LGBM\n",
        "lgbm_model = base_models[0][1]\n",
        "lgbm_model.fit(X_train, y_train)\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': lgbm_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "for idx, row in feature_importance.head(20).iterrows():\n",
        "    feat = row['feature']\n",
        "    # Mark feature type\n",
        "    if any(d in feat for d in ['execution_time', 'memory', 'cpu']):\n",
        "        marker = \"ðŸ”¥ [DYNAMIC]\"\n",
        "    elif any(s in feat for s in ['time_per', 'memory_per', 'cpu_']):\n",
        "        marker = \"âš¡ [HYBRID]\"\n",
        "    else:\n",
        "        marker = \"ðŸ“Š [STATIC] \"\n",
        "\n",
        "    print(f\"   {marker} {feat:40s} {row['importance']:8.1f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 13: DYNAMIC FEATURE CONTRIBUTION ANALYSIS\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸ”¬ Dynamic Feature Impact Analysis:\")\n",
        "\n",
        "# Count dynamic features in top 20\n",
        "top_20_features = feature_importance.head(20)['feature'].tolist()\n",
        "dynamic_in_top = sum(1 for f in top_20_features\n",
        "                     if any(d in f for d in ['execution_time', 'memory', 'cpu']))\n",
        "hybrid_in_top = sum(1 for f in top_20_features\n",
        "                    if any(h in f for h in ['time_per', 'memory_per', 'cpu_']))\n",
        "\n",
        "print(f\"   Dynamic features in Top 20: {dynamic_in_top}\")\n",
        "print(f\"   Hybrid features in Top 20: {hybrid_in_top}\")\n",
        "print(f\"   Static features in Top 20: {20 - dynamic_in_top - hybrid_in_top}\")\n",
        "\n",
        "# Total importance by type\n",
        "total_importance = feature_importance['importance'].sum()\n",
        "dynamic_importance = feature_importance[\n",
        "    feature_importance['feature'].str.contains('execution_time|memory_|cpu_', regex=True)\n",
        "]['importance'].sum()\n",
        "\n",
        "print(f\"\\n   Dynamic feature importance: {dynamic_importance:,.0f} ({dynamic_importance/total_importance*100:.1f}%)\")\n",
        "print(f\"   Static feature importance: {total_importance - dynamic_importance:,.0f} ({(1-dynamic_importance/total_importance)*100:.1f}%)\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 14: COMPARE WITH STATIC-ONLY\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸ“Š Performance Comparison:\")\n",
        "print(f\"   Static-only model (previous): RÂ² ~ 0.53-0.65\")\n",
        "print(f\"   Static + Dynamic (current):   RÂ² = {r2_test_log:.4f}\")\n",
        "print(f\"   Improvement: +{(r2_test_log - 0.59) * 100:.1f} percentage points\")\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL SUMMARY\n",
        "# ============================================================================\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(\"ðŸŽ‰ FINAL RESULTS - STATIC + DYNAMIC FEATURES\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"âœ… Samples: {X_train.shape[0]:,} train, {X_test.shape[0]:,} test\")\n",
        "print(f\"âœ… Features: {X_transformed.shape[1]} (static + dynamic, zero leakage)\")\n",
        "print(f\"âœ… Dynamic features used: {len(dynamic_used)}\")\n",
        "print(f\"âœ… Test RÂ² (Log): {r2_test_log:.6f}\")\n",
        "print(f\"âœ… Test RÂ² (Original): {r2_test_original:.6f}\")\n",
        "print(f\"âœ… CV RÂ²: {cv_scores.mean():.6f} Â±{cv_scores.std():.4f}\")\n",
        "print(f\"âœ… MAE: {mae_test:,.2f} J ({(mae_test/mean_energy)*100:.1f}%)\")\n",
        "print(f\"âœ… Overfitting: {r2_train_log - r2_test_log:.4f}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nðŸ”’ Leakage Prevention Summary:\")\n",
        "print(f\"   âœ… Excluded: {len(LEAKAGE_FEATURES)} leakage features\")\n",
        "print(f\"   âœ… Included: {len(dynamic_used)} clean dynamic features\")\n",
        "print(f\"   âœ… No energy-derived features used\")\n",
        "\n",
        "if r2_test_log >= 0.9:\n",
        "    print(\"\\nðŸŽŠ EXCELLENT: RÂ² â‰¥ 0.90 - Near-perfect prediction!\")\n",
        "elif r2_test_log >= 0.8:\n",
        "    print(\"\\nðŸš€ OUTSTANDING: RÂ² â‰¥ 0.80 - Production-ready!\")\n",
        "elif r2_test_log >= 0.7:\n",
        "    print(\"\\nâœ… STRONG: RÂ² â‰¥ 0.70 - Very good performance!\")\n",
        "else:\n",
        "    print(f\"\\nðŸ“Š Current: RÂ² = {r2_test_log:.3f}\")\n",
        "\n",
        "print(f\"\\nðŸ’¡ Key Insights:\")\n",
        "print(f\"   â€¢ Dynamic features explain {dynamic_importance/total_importance*100:.0f}% of prediction power\")\n",
        "print(f\"   â€¢ execution_time likely most important predictor\")\n",
        "print(f\"   â€¢ Model shows upper bound of predictability\")\n",
        "print(f\"   â€¢ Can be used for post-execution profiling/optimization\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "FlE2VtQEeCWq",
        "outputId": "722d643c-19bc-40db-d808-02dbb92ded46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STATIC + DYNAMIC FEATURES MODEL (ZERO LEAKAGE)\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š Initial Dataset: 3,212 rows Ã— 107 columns\n",
            "âœ… Valid samples: 626 rows\n",
            "\n",
            "ðŸ”’ Building feature set with STRICT leakage prevention...\n",
            "\n",
            "ðŸ“‹ Feature Breakdown:\n",
            "   Static features: 86\n",
            "   Dynamic features: 5\n",
            "   Gray features: 7\n",
            "   âœ… Total usable: 98\n",
            "   âœ… No leakage detected!\n",
            "\n",
            "ðŸ”¥ Dynamic features included:\n",
            "      â€¢ execution_time\n",
            "      â€¢ cpu_percent_avg\n",
            "      â€¢ cpu_percent_max\n",
            "      â€¢ memory_usage_mb\n",
            "      â€¢ memory_peak_mb\n",
            "\n",
            "ðŸ“ Feature Matrix: 626 rows Ã— 98 features\n",
            "\n",
            "ðŸ§¹ Data cleaning...\n",
            "   Removed 8 constant features\n",
            "âœ… Clean data: 618 samples, 90 features\n",
            "\n",
            "ðŸ”§ Feature Engineering...\n",
            "   â€¢ Static Ã— Dynamic interactions...\n",
            "   â€¢ Combined energy proxy...\n",
            "   â€¢ Static feature polynomials...\n",
            "âœ… Engineered features: 121 total\n",
            "\n",
            "ðŸ“ˆ Target (log): [1.59, 6.07]\n",
            "\n",
            "ðŸŽ¯ Feature Selection...\n",
            "âœ… Selected 80 features\n",
            "\n",
            "ðŸ“Š Split: 525 train, 93 test\n",
            "\n",
            "ðŸš€ Training Stacking Ensemble (Static + Dynamic Features)...\n",
            "   Training ensemble...\n",
            "\n",
            "================================================================================\n",
            "MODEL PERFORMANCE: STATIC + DYNAMIC (NO LEAKAGE)\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š Log-Scale Performance:\n",
            "   Training RÂ²: 0.999959\n",
            "   Test RÂ²:     0.999875\n",
            "   Overfitting Gap: 0.000084\n",
            "\n",
            "âš¡ Original Joules Scale:\n",
            "   Training RÂ²: 0.999622\n",
            "   Test RÂ²:     0.998769\n",
            "\n",
            "ðŸ“ Error Metrics:\n",
            "   Test MAE:  0.95 J (1.8% of mean)\n",
            "   Test RMSE: 3.19 J\n",
            "   Mean Energy: 52.51 J\n",
            "\n",
            "ðŸ”„ 5-Fold Cross-Validation:\n",
            "   CV RÂ²: ['0.9998', '0.9999', '0.9999', '0.9997', '0.9999']\n",
            "   Mean: 0.999856 (Â±0.000085)\n",
            "\n",
            "ðŸŽ¯ Top 20 Features (Static vs Dynamic):\n",
            "--------------------------------------------------------------------------------\n",
            "   ðŸ”¥ [DYNAMIC] execution_time                             5333.0\n",
            "   ðŸ”¥ [DYNAMIC] cpu_variability                            1113.0\n",
            "   ðŸ“Š [STATIC]  code_ratio                                  912.0\n",
            "   ðŸ”¥ [DYNAMIC] cpu_percent_avg                             844.0\n",
            "   âš¡ [HYBRID] time_per_line_calc                          717.0\n",
            "   âš¡ [HYBRID] time_per_loop                               615.0\n",
            "   ðŸ“Š [STATIC]  time_squared                                528.0\n",
            "   ðŸ”¥ [DYNAMIC] cpu_time_product                            520.0\n",
            "   ðŸ“Š [STATIC]  avg_line_length                             503.0\n",
            "   ðŸ“Š [STATIC]  std_line_length                             489.0\n",
            "   âš¡ [HYBRID] time_per_complexity                         471.0\n",
            "   ðŸ“Š [STATIC]  combined_energy_proxy                       425.0\n",
            "   ðŸ”¥ [DYNAMIC] memory_usage_mb                             423.0\n",
            "   ðŸ“Š [STATIC]  assignments                                 387.0\n",
            "   ðŸ”¥ [DYNAMIC] memory_per_line                             260.0\n",
            "   ðŸ“Š [STATIC]  single_quotes                               223.0\n",
            "   ðŸ“Š [STATIC]  colon_count                                 215.0\n",
            "   ðŸ“Š [STATIC]  functions_per_line                          213.0\n",
            "   ðŸ“Š [STATIC]  comment_ratio_calc                          212.0\n",
            "   ðŸ”¥ [DYNAMIC] cpu_complexity_ratio                        208.0\n",
            "\n",
            "ðŸ”¬ Dynamic Feature Impact Analysis:\n",
            "   Dynamic features in Top 20: 7\n",
            "   Hybrid features in Top 20: 8\n",
            "   Static features in Top 20: 5\n",
            "\n",
            "   Dynamic feature importance: 9,169 (52.4%)\n",
            "   Static feature importance: 8,333 (47.6%)\n",
            "\n",
            "ðŸ“Š Performance Comparison:\n",
            "   Static-only model (previous): RÂ² ~ 0.53-0.65\n",
            "   Static + Dynamic (current):   RÂ² = 0.9999\n",
            "   Improvement: +41.0 percentage points\n",
            "\n",
            "================================================================================\n",
            "ðŸŽ‰ FINAL RESULTS - STATIC + DYNAMIC FEATURES\n",
            "================================================================================\n",
            "âœ… Samples: 525 train, 93 test\n",
            "âœ… Features: 80 (static + dynamic, zero leakage)\n",
            "âœ… Dynamic features used: 5\n",
            "âœ… Test RÂ² (Log): 0.999875\n",
            "âœ… Test RÂ² (Original): 0.998769\n",
            "âœ… CV RÂ²: 0.999856 Â±0.0001\n",
            "âœ… MAE: 0.95 J (1.8%)\n",
            "âœ… Overfitting: 0.0001\n",
            "================================================================================\n",
            "\n",
            "ðŸ”’ Leakage Prevention Summary:\n",
            "   âœ… Excluded: 7 leakage features\n",
            "   âœ… Included: 5 clean dynamic features\n",
            "   âœ… No energy-derived features used\n",
            "\n",
            "ðŸŽŠ EXCELLENT: RÂ² â‰¥ 0.90 - Near-perfect prediction!\n",
            "\n",
            "ðŸ’¡ Key Insights:\n",
            "   â€¢ Dynamic features explain 52% of prediction power\n",
            "   â€¢ execution_time likely most important predictor\n",
            "   â€¢ Model shows upper bound of predictability\n",
            "   â€¢ Can be used for post-execution profiling/optimization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#HYBRID\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.linear_model import Ridge, ElasticNet\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "DATASET_PATH = '/content/ml_code_dataset.csv'\n",
        "\n",
        "# ============================================================================\n",
        "# FEATURE CATEGORIZATION - STRICT LEAKAGE PREVENTION\n",
        "# ============================================================================\n",
        "\n",
        "# TARGET (what we're predicting)\n",
        "TARGET = 'energy_consumption_joules'\n",
        "\n",
        "# LEAKAGE FEATURES - MUST BE REMOVED\n",
        "# These are derived FROM the target or highly correlated outcomes\n",
        "LEAKAGE_FEATURES = [\n",
        "    'co2_emissions_g',           # Direct calculation from energy\n",
        "    'energy_per_line',           # Derived: energy / lines\n",
        "    'time_per_line',             # Derived: time / lines\n",
        "    'co2_per_line',              # Derived: co2 / lines\n",
        "    'energy_pkg_joules',         # Alternative energy measurement\n",
        "    'energy_ram_joules',         # Alternative energy measurement\n",
        "    'power_draw_watts',          # Direct energy measurement\n",
        "]\n",
        "\n",
        "# DYNAMIC FEATURES - CAN USE (measured during execution, but not derived from energy)\n",
        "DYNAMIC_FEATURES = [\n",
        "    'execution_time',            # Independent measurement\n",
        "    'cpu_percent_avg',           # Independent measurement\n",
        "    'cpu_percent_max',           # Independent measurement\n",
        "    'memory_usage_mb',           # Independent measurement\n",
        "    'memory_peak_mb',            # Independent measurement\n",
        "]\n",
        "\n",
        "# GRAY FEATURES - CAN USE\n",
        "GRAY_FEATURES = [\n",
        "    'executed_successfully', 'execution_error', 'exit_code',\n",
        "    'syntax_valid', 'parse_error', 'syntax_error_line', 'syntax_error_type'\n",
        "]\n",
        "\n",
        "# STATIC FEATURES - CAN USE\n",
        "STATIC_FEATURES = [\n",
        "    'total_lines', 'code_lines', 'blank_lines', 'total_characters',\n",
        "    'file_size_bytes', 'file_size_kb', 'avg_line_length', 'max_line_length',\n",
        "    'min_line_length', 'std_line_length', 'comment_lines', 'single_quotes',\n",
        "    'double_quotes', 'triple_quotes', 'total_quotes', 'parentheses_count',\n",
        "    'brackets_count', 'braces_count', 'semicolon_count', 'colon_count',\n",
        "    'unique_indentation_levels', 'avg_indentation', 'max_indentation',\n",
        "    'std_indentation', 'keyword_def_count', 'keyword_class_count',\n",
        "    'keyword_import_count', 'keyword_from_count', 'keyword_if_count',\n",
        "    'keyword_else_count', 'keyword_elif_count', 'keyword_for_count',\n",
        "    'keyword_while_count', 'keyword_try_count', 'keyword_except_count',\n",
        "    'keyword_finally_count', 'keyword_with_count', 'keyword_return_count',\n",
        "    'keyword_yield_count', 'keyword_lambda_count', 'keyword_async_count',\n",
        "    'keyword_await_count', 'keyword_global_count', 'keyword_nonlocal_count',\n",
        "    'functions_count', 'async_functions_count', 'classes_count',\n",
        "    'imports_count', 'if_statements', 'for_loops', 'while_loops',\n",
        "    'async_for_loops', 'try_blocks', 'with_statements', 'async_with_statements',\n",
        "    'lambda_functions', 'list_comprehensions', 'dict_comprehensions',\n",
        "    'set_comprehensions', 'generator_expressions', 'binary_operations',\n",
        "    'unary_operations', 'boolean_operations', 'comparisons', 'decorators',\n",
        "    'assignments', 'augmented_assignments', 'return_statements',\n",
        "    'yield_statements', 'raise_statements', 'assert_statements',\n",
        "    'cyclomatic_complexity', 'max_nesting_depth', 'unique_function_names',\n",
        "    'unique_class_names', 'unique_imports', 'unique_variables',\n",
        "    'total_ast_nodes', 'comment_ratio', 'control_flow_ratio',\n",
        "    'function_ratio', 'class_ratio', 'code_density', 'complexity_per_line',\n",
        "    'functions_per_line', 'avg_complexity_per_function'\n",
        "]\n",
        "\n",
        "# METADATA - EXCLUDE\n",
        "METADATA_COLS = [\n",
        "    'file_path', 'file_name', 'repo_name', 'repo_url', 'timestamp',\n",
        "    'run_id', 'index', 'id', 'file_id', 'repository', 'author',\n",
        "    'date_created', 'date_modified', 'file_index'\n",
        "]\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: LOAD DATA\n",
        "# ============================================================================\n",
        "print(\"=\" * 80)\n",
        "print(\"STATIC + DYNAMIC FEATURES MODEL (ZERO LEAKAGE)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "print(f\"\\nðŸ“Š Initial Dataset: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
        "\n",
        "if 'executed_successfully' in df.columns:\n",
        "    df = df[df['executed_successfully'] == True].copy()\n",
        "\n",
        "df = df[df[TARGET] > 0].copy()\n",
        "df = df.dropna(subset=[TARGET])\n",
        "print(f\"âœ… Valid samples: {df.shape[0]:,} rows\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: BUILD FEATURE SET (PREVENT LEAKAGE!)\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸ”’ Building feature set with STRICT leakage prevention...\")\n",
        "\n",
        "available_cols = df.columns.tolist()\n",
        "\n",
        "# Collect all usable features\n",
        "usable_features = []\n",
        "for col in available_cols:\n",
        "    # Include if it's static, dynamic, or gray\n",
        "    if col in STATIC_FEATURES or col in DYNAMIC_FEATURES or col in GRAY_FEATURES:\n",
        "        # Exclude if it's target, metadata, or leakage\n",
        "        if col not in [TARGET] + METADATA_COLS + LEAKAGE_FEATURES:\n",
        "            usable_features.append(col)\n",
        "\n",
        "# Categorize features\n",
        "static_used = [f for f in usable_features if f in STATIC_FEATURES]\n",
        "dynamic_used = [f for f in usable_features if f in DYNAMIC_FEATURES]\n",
        "gray_used = [f for f in usable_features if f in GRAY_FEATURES]\n",
        "\n",
        "print(f\"\\nðŸ“‹ Feature Breakdown:\")\n",
        "print(f\"   Static features: {len(static_used)}\")\n",
        "print(f\"   Dynamic features: {len(dynamic_used)}\")\n",
        "print(f\"   Gray features: {len(gray_used)}\")\n",
        "print(f\"   âœ… Total usable: {len(usable_features)}\")\n",
        "\n",
        "# Verify no leakage\n",
        "leaked = [f for f in usable_features if f in LEAKAGE_FEATURES]\n",
        "if leaked:\n",
        "    print(f\"   âŒ ERROR: Leaked features found: {leaked}\")\n",
        "    exit()\n",
        "else:\n",
        "    print(f\"   âœ… No leakage detected!\")\n",
        "\n",
        "# Show dynamic features being used\n",
        "if dynamic_used:\n",
        "    print(f\"\\nðŸ”¥ Dynamic features included:\")\n",
        "    for feat in dynamic_used:\n",
        "        print(f\"      â€¢ {feat}\")\n",
        "\n",
        "X = df[usable_features].copy()\n",
        "y = df[TARGET].copy()\n",
        "\n",
        "print(f\"\\nðŸ“ Feature Matrix: {X.shape[0]:,} rows Ã— {X.shape[1]} features\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: MINIMAL CLEANING\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸ§¹ Data cleaning...\")\n",
        "\n",
        "# Handle infinite/NaN\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "for col in X.columns:\n",
        "    if X[col].dtype in ['float64', 'int64']:\n",
        "        if X[col].isna().sum() > 0:\n",
        "            X[col].fillna(X[col].median(), inplace=True)\n",
        "\n",
        "# Remove constant features\n",
        "constant_cols = [col for col in X.columns if X[col].nunique() <= 1]\n",
        "if constant_cols:\n",
        "    X = X.drop(columns=constant_cols)\n",
        "    print(f\"   Removed {len(constant_cols)} constant features\")\n",
        "\n",
        "# Very minimal outlier removal\n",
        "q_low = y.quantile(0.005)\n",
        "q_high = y.quantile(0.995)\n",
        "mask = (y >= q_low) & (y <= q_high)\n",
        "X = X[mask]\n",
        "y = y[mask]\n",
        "\n",
        "print(f\"âœ… Clean data: {X.shape[0]:,} samples, {X.shape[1]} features\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: FEATURE ENGINEERING\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸ”§ Feature Engineering...\")\n",
        "\n",
        "X_eng = X.copy()\n",
        "\n",
        "# === INTERACTIONS BETWEEN STATIC AND DYNAMIC ===\n",
        "print(\"   â€¢ Static Ã— Dynamic interactions...\")\n",
        "\n",
        "# Execution time interactions\n",
        "if 'execution_time' in X_eng.columns:\n",
        "    if 'total_lines' in X_eng.columns:\n",
        "        X_eng['time_per_line_calc'] = X_eng['execution_time'] / (X_eng['total_lines'] + 1)\n",
        "    if 'cyclomatic_complexity' in X_eng.columns:\n",
        "        X_eng['time_per_complexity'] = X_eng['execution_time'] / (X_eng['cyclomatic_complexity'] + 1)\n",
        "    if 'for_loops' in X_eng.columns:\n",
        "        X_eng['time_per_loop'] = X_eng['execution_time'] / (X_eng['for_loops'] + 1)\n",
        "\n",
        "    # Time efficiency ratios\n",
        "    X_eng['time_squared'] = X_eng['execution_time'] ** 2\n",
        "    X_eng['time_log'] = np.log1p(X_eng['execution_time'])\n",
        "    X_eng['time_sqrt'] = np.sqrt(X_eng['execution_time'])\n",
        "\n",
        "# Memory interactions\n",
        "if 'memory_peak_mb' in X_eng.columns:\n",
        "    if 'total_lines' in X_eng.columns:\n",
        "        X_eng['memory_per_line'] = X_eng['memory_peak_mb'] / (X_eng['total_lines'] + 1)\n",
        "    if 'unique_variables' in X_eng.columns:\n",
        "        X_eng['memory_per_var'] = X_eng['memory_peak_mb'] / (X_eng['unique_variables'] + 1)\n",
        "    if 'classes_count' in X_eng.columns:\n",
        "        X_eng['memory_per_class'] = X_eng['memory_peak_mb'] / (X_eng['classes_count'] + 1)\n",
        "\n",
        "    X_eng['memory_squared'] = X_eng['memory_peak_mb'] ** 2\n",
        "    X_eng['memory_log'] = np.log1p(X_eng['memory_peak_mb'])\n",
        "\n",
        "# CPU interactions\n",
        "if 'cpu_percent_avg' in X_eng.columns:\n",
        "    if 'execution_time' in X_eng.columns:\n",
        "        X_eng['cpu_time_product'] = X_eng['cpu_percent_avg'] * X_eng['execution_time']\n",
        "    if 'cyclomatic_complexity' in X_eng.columns:\n",
        "        X_eng['cpu_complexity_ratio'] = X_eng['cpu_percent_avg'] / (X_eng['cyclomatic_complexity'] + 1)\n",
        "\n",
        "if 'cpu_percent_max' in X_eng.columns and 'cpu_percent_avg' in X_eng.columns:\n",
        "    X_eng['cpu_variability'] = X_eng['cpu_percent_max'] - X_eng['cpu_percent_avg']\n",
        "\n",
        "# === COMBINED ENERGY PROXY ===\n",
        "print(\"   â€¢ Combined energy proxy...\")\n",
        "\n",
        "energy_proxy = 0\n",
        "if 'execution_time' in X_eng.columns:\n",
        "    energy_proxy += X_eng['execution_time'] * 50  # Time is major factor\n",
        "\n",
        "if 'memory_peak_mb' in X_eng.columns:\n",
        "    energy_proxy += X_eng['memory_peak_mb'] * 0.5  # Memory contributes\n",
        "\n",
        "if 'cpu_percent_avg' in X_eng.columns:\n",
        "    energy_proxy += X_eng['cpu_percent_avg'] * 2  # CPU contributes\n",
        "\n",
        "if isinstance(energy_proxy, pd.Series):\n",
        "    X_eng['combined_energy_proxy'] = energy_proxy\n",
        "\n",
        "# === STATIC FEATURE ENGINEERING ===\n",
        "print(\"   â€¢ Static feature polynomials...\")\n",
        "\n",
        "for col in ['total_lines', 'code_lines', 'cyclomatic_complexity', 'total_ast_nodes']:\n",
        "    if col in X_eng.columns:\n",
        "        X_eng[f'{col}_sqrt'] = np.sqrt(X_eng[col] + 1)\n",
        "        X_eng[f'{col}_log'] = np.log1p(X_eng[col])\n",
        "        X_eng[f'{col}_square'] = X_eng[col] ** 2\n",
        "\n",
        "# Static interactions\n",
        "if 'total_lines' in X_eng.columns and 'cyclomatic_complexity' in X_eng.columns:\n",
        "    X_eng['lines_x_complexity'] = X_eng['total_lines'] * X_eng['cyclomatic_complexity']\n",
        "\n",
        "if 'code_lines' in X_eng.columns and 'total_ast_nodes' in X_eng.columns:\n",
        "    X_eng['code_x_ast'] = X_eng['code_lines'] * X_eng['total_ast_nodes']\n",
        "\n",
        "# Ratios\n",
        "if 'code_lines' in X_eng.columns and 'total_lines' in X_eng.columns:\n",
        "    X_eng['code_ratio'] = X_eng['code_lines'] / (X_eng['total_lines'] + 1)\n",
        "\n",
        "if 'comment_lines' in X_eng.columns and 'code_lines' in X_eng.columns:\n",
        "    X_eng['comment_ratio_calc'] = X_eng['comment_lines'] / (X_eng['code_lines'] + 1)\n",
        "\n",
        "print(f\"âœ… Engineered features: {X_eng.shape[1]} total\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: LOG TRANSFORM TARGET\n",
        "# ============================================================================\n",
        "y_log = np.log1p(y)\n",
        "print(f\"\\nðŸ“ˆ Target (log): [{y_log.min():.2f}, {y_log.max():.2f}]\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: FEATURE SELECTION\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸŽ¯ Feature Selection...\")\n",
        "\n",
        "k_best = min(80, X_eng.shape[1])\n",
        "selector = SelectKBest(score_func=mutual_info_regression, k=k_best)\n",
        "selector.fit(X_eng, y_log)\n",
        "X_selected = X_eng.loc[:, selector.get_support()]\n",
        "\n",
        "print(f\"âœ… Selected {X_selected.shape[1]} features\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: NORMALIZE\n",
        "# ============================================================================\n",
        "qt = QuantileTransformer(\n",
        "    n_quantiles=min(1000, X_selected.shape[0]),\n",
        "    output_distribution='normal',\n",
        "    random_state=42\n",
        ")\n",
        "X_transformed = pd.DataFrame(\n",
        "    qt.fit_transform(X_selected),\n",
        "    columns=X_selected.columns,\n",
        "    index=X_selected.index\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 8: STRATIFIED SPLIT\n",
        "# ============================================================================\n",
        "y_bins = pd.qcut(y_log, q=5, labels=False, duplicates='drop')\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_transformed, y_log, test_size=0.15, random_state=42,\n",
        "    stratify=y_bins, shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"\\nðŸ“Š Split: {X_train.shape[0]:,} train, {X_test.shape[0]:,} test\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 9: OPTIMIZED STACKING ENSEMBLE\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸš€ Training Stacking Ensemble (Static + Dynamic Features)...\")\n",
        "\n",
        "# With dynamic features, we can use less regularization\n",
        "base_models = [\n",
        "    ('lgbm1', LGBMRegressor(\n",
        "        n_estimators=1500, learning_rate=0.01, num_leaves=50,\n",
        "        max_depth=10, min_child_samples=10, subsample=0.9,\n",
        "        colsample_bytree=0.9, reg_alpha=0.1, reg_lambda=0.1,\n",
        "        random_state=42, verbose=-1, n_jobs=-1\n",
        "    )),\n",
        "    ('lgbm2', LGBMRegressor(\n",
        "        n_estimators=1200, learning_rate=0.015, num_leaves=35,\n",
        "        max_depth=8, min_child_samples=15, subsample=0.85,\n",
        "        colsample_bytree=0.85, reg_alpha=0.3, reg_lambda=0.3,\n",
        "        random_state=43, verbose=-1, n_jobs=-1\n",
        "    )),\n",
        "    ('xgb1', XGBRegressor(\n",
        "        n_estimators=1500, learning_rate=0.01, max_depth=10,\n",
        "        min_child_weight=1, subsample=0.9, colsample_bytree=0.9,\n",
        "        reg_alpha=0.1, reg_lambda=0.1, random_state=42,\n",
        "        verbosity=0, n_jobs=-1\n",
        "    )),\n",
        "    ('xgb2', XGBRegressor(\n",
        "        n_estimators=1200, learning_rate=0.015, max_depth=8,\n",
        "        min_child_weight=2, subsample=0.85, colsample_bytree=0.85,\n",
        "        reg_alpha=0.3, reg_lambda=0.3, random_state=43,\n",
        "        verbosity=0, n_jobs=-1\n",
        "    )),\n",
        "    ('ridge', Ridge(alpha=1.0)),\n",
        "    ('elastic', ElasticNet(alpha=0.001, l1_ratio=0.5, max_iter=5000))\n",
        "]\n",
        "\n",
        "meta_model = Ridge(alpha=1.0)\n",
        "\n",
        "stacking_model = StackingRegressor(\n",
        "    estimators=base_models,\n",
        "    final_estimator=meta_model,\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"   Training ensemble...\")\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 10: EVALUATE\n",
        "# ============================================================================\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(\"MODEL PERFORMANCE: STATIC + DYNAMIC (NO LEAKAGE)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "y_train_pred_log = stacking_model.predict(X_train)\n",
        "y_test_pred_log = stacking_model.predict(X_test)\n",
        "\n",
        "r2_train_log = r2_score(y_train, y_train_pred_log)\n",
        "r2_test_log = r2_score(y_test, y_test_pred_log)\n",
        "\n",
        "print(f\"\\nðŸ“Š Log-Scale Performance:\")\n",
        "print(f\"   Training RÂ²: {r2_train_log:.6f}\")\n",
        "print(f\"   Test RÂ²:     {r2_test_log:.6f}\")\n",
        "print(f\"   Overfitting Gap: {r2_train_log - r2_test_log:.6f}\")\n",
        "\n",
        "# Original scale\n",
        "y_train_actual = np.expm1(y_train)\n",
        "y_train_pred = np.expm1(y_train_pred_log)\n",
        "y_test_actual = np.expm1(y_test)\n",
        "y_test_pred = np.expm1(y_test_pred_log)\n",
        "\n",
        "r2_train_original = r2_score(y_train_actual, y_train_pred)\n",
        "r2_test_original = r2_score(y_test_actual, y_test_pred)\n",
        "\n",
        "print(f\"\\nâš¡ Original Joules Scale:\")\n",
        "print(f\"   Training RÂ²: {r2_train_original:.6f}\")\n",
        "print(f\"   Test RÂ²:     {r2_test_original:.6f}\")\n",
        "\n",
        "mae_test = mean_absolute_error(y_test_actual, y_test_pred)\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test_actual, y_test_pred))\n",
        "mean_energy = y_test_actual.mean()\n",
        "\n",
        "print(f\"\\nðŸ“ Error Metrics:\")\n",
        "print(f\"   Test MAE:  {mae_test:,.2f} J ({(mae_test/mean_energy)*100:.1f}% of mean)\")\n",
        "print(f\"   Test RMSE: {rmse_test:,.2f} J\")\n",
        "print(f\"   Mean Energy: {mean_energy:,.2f} J\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 11: CROSS-VALIDATION\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸ”„ 5-Fold Cross-Validation:\")\n",
        "cv_scores = cross_val_score(\n",
        "    stacking_model, X_transformed, y_log, cv=5,\n",
        "    scoring='r2', n_jobs=-1\n",
        ")\n",
        "print(f\"   CV RÂ²: {[f'{s:.4f}' for s in cv_scores]}\")\n",
        "print(f\"   Mean: {cv_scores.mean():.6f} (Â±{cv_scores.std():.6f})\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 12: FEATURE IMPORTANCE ANALYSIS\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸŽ¯ Top 20 Features (Static vs Dynamic):\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Get importance from LGBM\n",
        "lgbm_model = base_models[0][1]\n",
        "lgbm_model.fit(X_train, y_train)\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': lgbm_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "for idx, row in feature_importance.head(20).iterrows():\n",
        "    feat = row['feature']\n",
        "    # Mark feature type\n",
        "    if any(d in feat for d in ['execution_time', 'memory', 'cpu']):\n",
        "        marker = \"ðŸ”¥ [DYNAMIC]\"\n",
        "    elif any(s in feat for s in ['time_per', 'memory_per', 'cpu_']):\n",
        "        marker = \"âš¡ [HYBRID]\"\n",
        "    else:\n",
        "        marker = \"ðŸ“Š [STATIC] \"\n",
        "\n",
        "    print(f\"   {marker} {feat:40s} {row['importance']:8.1f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 13: DYNAMIC FEATURE CONTRIBUTION ANALYSIS\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸ”¬ Dynamic Feature Impact Analysis:\")\n",
        "\n",
        "# Count dynamic features in top 20\n",
        "top_20_features = feature_importance.head(20)['feature'].tolist()\n",
        "dynamic_in_top = sum(1 for f in top_20_features\n",
        "                     if any(d in f for d in ['execution_time', 'memory', 'cpu']))\n",
        "hybrid_in_top = sum(1 for f in top_20_features\n",
        "                    if any(h in f for h in ['time_per', 'memory_per', 'cpu_']))\n",
        "\n",
        "print(f\"   Dynamic features in Top 20: {dynamic_in_top}\")\n",
        "print(f\"   Hybrid features in Top 20: {hybrid_in_top}\")\n",
        "print(f\"   Static features in Top 20: {20 - dynamic_in_top - hybrid_in_top}\")\n",
        "\n",
        "# Total importance by type\n",
        "total_importance = feature_importance['importance'].sum()\n",
        "dynamic_importance = feature_importance[\n",
        "    feature_importance['feature'].str.contains('execution_time|memory_|cpu_', regex=True)\n",
        "]['importance'].sum()\n",
        "\n",
        "print(f\"\\n   Dynamic feature importance: {dynamic_importance:,.0f} ({dynamic_importance/total_importance*100:.1f}%)\")\n",
        "print(f\"   Static feature importance: {total_importance - dynamic_importance:,.0f} ({(1-dynamic_importance/total_importance)*100:.1f}%)\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 14: COMPARE WITH STATIC-ONLY\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸ“Š Performance Comparison:\")\n",
        "print(f\"   Static-only model (previous): RÂ² ~ 0.53-0.65\")\n",
        "print(f\"   Static + Dynamic (current):   RÂ² = {r2_test_log:.4f}\")\n",
        "print(f\"   Improvement: +{(r2_test_log - 0.59) * 100:.1f} percentage points\")\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL SUMMARY\n",
        "# ============================================================================\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(\"ðŸŽ‰ FINAL RESULTS - STATIC + DYNAMIC FEATURES\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"âœ… Samples: {X_train.shape[0]:,} train, {X_test.shape[0]:,} test\")\n",
        "print(f\"âœ… Features: {X_transformed.shape[1]} (static + dynamic, zero leakage)\")\n",
        "print(f\"âœ… Dynamic features used: {len(dynamic_used)}\")\n",
        "print(f\"âœ… Test RÂ² (Log): {r2_test_log:.6f}\")\n",
        "print(f\"âœ… Test RÂ² (Original): {r2_test_original:.6f}\")\n",
        "print(f\"âœ… CV RÂ²: {cv_scores.mean():.6f} Â±{cv_scores.std():.4f}\")\n",
        "print(f\"âœ… MAE: {mae_test:,.2f} J ({(mae_test/mean_energy)*100:.1f}%)\")\n",
        "print(f\"âœ… Overfitting: {r2_train_log - r2_test_log:.4f}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\nðŸ”’ Leakage Prevention Summary:\")\n",
        "print(f\"   âœ… Excluded: {len(LEAKAGE_FEATURES)} leakage features\")\n",
        "print(f\"   âœ… Included: {len(dynamic_used)} clean dynamic features\")\n",
        "print(f\"   âœ… No energy-derived features used\")\n",
        "\n",
        "if r2_test_log >= 0.9:\n",
        "    print(\"\\nðŸŽŠ EXCELLENT: RÂ² â‰¥ 0.90 - Near-perfect prediction!\")\n",
        "elif r2_test_log >= 0.8:\n",
        "    print(\"\\nðŸš€ OUTSTANDING: RÂ² â‰¥ 0.80 - Production-ready!\")\n",
        "elif r2_test_log >= 0.7:\n",
        "    print(\"\\nâœ… STRONG: RÂ² â‰¥ 0.70 - Very good performance!\")\n",
        "else:\n",
        "    print(f\"\\nðŸ“Š Current: RÂ² = {r2_test_log:.3f}\")\n",
        "\n",
        "print(f\"\\nðŸ’¡ Key Insights:\")\n",
        "print(f\"   â€¢ Dynamic features explain {dynamic_importance/total_importance*100:.0f}% of prediction power\")\n",
        "print(f\"   â€¢ execution_time likely most important predictor\")\n",
        "print(f\"   â€¢ Model shows upper bound of predictability\")\n",
        "print(f\"   â€¢ Can be used for post-execution profiling/optimization\")"
      ],
      "metadata": {
        "id": "ltYn3upyb66p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "098ccafd-8b67-4cc6-f99b-e4cff1901edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "STATIC + DYNAMIC FEATURES MODEL (ZERO LEAKAGE)\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š Initial Dataset: 6,711 rows Ã— 107 columns\n",
            "âœ… Valid samples: 1,567 rows\n",
            "\n",
            "ðŸ”’ Building feature set with STRICT leakage prevention...\n",
            "\n",
            "ðŸ“‹ Feature Breakdown:\n",
            "   Static features: 86\n",
            "   Dynamic features: 5\n",
            "   Gray features: 7\n",
            "   âœ… Total usable: 98\n",
            "   âœ… No leakage detected!\n",
            "\n",
            "ðŸ”¥ Dynamic features included:\n",
            "      â€¢ execution_time\n",
            "      â€¢ cpu_percent_avg\n",
            "      â€¢ cpu_percent_max\n",
            "      â€¢ memory_usage_mb\n",
            "      â€¢ memory_peak_mb\n",
            "\n",
            "ðŸ“ Feature Matrix: 1,567 rows Ã— 98 features\n",
            "\n",
            "ðŸ§¹ Data cleaning...\n",
            "   Removed 6 constant features\n",
            "âœ… Clean data: 1,551 samples, 92 features\n",
            "\n",
            "ðŸ”§ Feature Engineering...\n",
            "   â€¢ Static Ã— Dynamic interactions...\n",
            "   â€¢ Combined energy proxy...\n",
            "   â€¢ Static feature polynomials...\n",
            "âœ… Engineered features: 123 total\n",
            "\n",
            "ðŸ“ˆ Target (log): [1.46, 6.18]\n",
            "\n",
            "ðŸŽ¯ Feature Selection...\n",
            "âœ… Selected 80 features\n",
            "\n",
            "ðŸ“Š Split: 1,318 train, 233 test\n",
            "\n",
            "ðŸš€ Training Stacking Ensemble (Static + Dynamic Features)...\n",
            "   Training ensemble...\n",
            "\n",
            "================================================================================\n",
            "MODEL PERFORMANCE: STATIC + DYNAMIC (NO LEAKAGE)\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š Log-Scale Performance:\n",
            "   Training RÂ²: 0.999988\n",
            "   Test RÂ²:     0.999962\n",
            "   Overfitting Gap: 0.000026\n",
            "\n",
            "âš¡ Original Joules Scale:\n",
            "   Training RÂ²: 0.999897\n",
            "   Test RÂ²:     0.999743\n",
            "\n",
            "ðŸ“ Error Metrics:\n",
            "   Test MAE:  0.47 J (0.9% of mean)\n",
            "   Test RMSE: 1.35 J\n",
            "   Mean Energy: 54.32 J\n",
            "\n",
            "ðŸ”„ 5-Fold Cross-Validation:\n",
            "   CV RÂ²: ['1.0000', '1.0000', '1.0000', '1.0000', '1.0000']\n",
            "   Mean: 0.999962 (Â±0.000004)\n",
            "\n",
            "ðŸŽ¯ Top 20 Features (Static vs Dynamic):\n",
            "--------------------------------------------------------------------------------\n",
            "   ðŸ”¥ [DYNAMIC] execution_time                             7399.0\n",
            "   ðŸ”¥ [DYNAMIC] cpu_time_product                           1515.0\n",
            "   âš¡ [HYBRID] time_per_loop                              1175.0\n",
            "   ðŸ“Š [STATIC]  std_line_length                            1046.0\n",
            "   ðŸ”¥ [DYNAMIC] cpu_variability                            1009.0\n",
            "   ðŸ“Š [STATIC]  combined_energy_proxy                      1001.0\n",
            "   âš¡ [HYBRID] time_per_complexity                         851.0\n",
            "   ðŸ“Š [STATIC]  function_ratio                              839.0\n",
            "   ðŸ“Š [STATIC]  complexity_per_line                         831.0\n",
            "   ðŸ“Š [STATIC]  time_squared                                766.0\n",
            "   ðŸ“Š [STATIC]  single_quotes                               765.0\n",
            "   ðŸ“Š [STATIC]  total_quotes                                757.0\n",
            "   ðŸ”¥ [DYNAMIC] memory_usage_mb                             687.0\n",
            "   âš¡ [HYBRID] time_per_line_calc                          647.0\n",
            "   ðŸ“Š [STATIC]  comment_ratio                               625.0\n",
            "   ðŸ“Š [STATIC]  control_flow_ratio                          621.0\n",
            "   ðŸ”¥ [DYNAMIC] cpu_complexity_ratio                        552.0\n",
            "   ðŸ“Š [STATIC]  unary_operations                            550.0\n",
            "   ðŸ“Š [STATIC]  code_ratio                                  537.0\n",
            "   ðŸ“Š [STATIC]  double_quotes                               531.0\n",
            "\n",
            "ðŸ”¬ Dynamic Feature Impact Analysis:\n",
            "   Dynamic features in Top 20: 5\n",
            "   Hybrid features in Top 20: 6\n",
            "   Static features in Top 20: 9\n",
            "\n",
            "   Dynamic feature importance: 13,155 (40.3%)\n",
            "   Static feature importance: 19,507 (59.7%)\n",
            "\n",
            "ðŸ“Š Performance Comparison:\n",
            "   Static-only model (previous): RÂ² ~ 0.53-0.65\n",
            "   Static + Dynamic (current):   RÂ² = 1.0000\n",
            "   Improvement: +41.0 percentage points\n",
            "\n",
            "================================================================================\n",
            "ðŸŽ‰ FINAL RESULTS - STATIC + DYNAMIC FEATURES\n",
            "================================================================================\n",
            "âœ… Samples: 1,318 train, 233 test\n",
            "âœ… Features: 80 (static + dynamic, zero leakage)\n",
            "âœ… Dynamic features used: 5\n",
            "âœ… Test RÂ² (Log): 0.999962\n",
            "âœ… Test RÂ² (Original): 0.999743\n",
            "âœ… CV RÂ²: 0.999962 Â±0.0000\n",
            "âœ… MAE: 0.47 J (0.9%)\n",
            "âœ… Overfitting: 0.0000\n",
            "================================================================================\n",
            "\n",
            "ðŸ”’ Leakage Prevention Summary:\n",
            "   âœ… Excluded: 7 leakage features\n",
            "   âœ… Included: 5 clean dynamic features\n",
            "   âœ… No energy-derived features used\n",
            "\n",
            "ðŸŽŠ EXCELLENT: RÂ² â‰¥ 0.90 - Near-perfect prediction!\n",
            "\n",
            "ðŸ’¡ Key Insights:\n",
            "   â€¢ Dynamic features explain 40% of prediction power\n",
            "   â€¢ execution_time likely most important predictor\n",
            "   â€¢ Model shows upper bound of predictability\n",
            "   â€¢ Can be used for post-execution profiling/optimization\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================================================\n",
        "# FINAL FULL CODE â€” HYBRID (STATIC + PROXY) WITH RANDOM FOREST\n",
        "# ===============================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import StackingRegressor, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.linear_model import Ridge, ElasticNet\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ===============================================================\n",
        "# CONFIG\n",
        "# ===============================================================\n",
        "\n",
        "DATASET_PATH = \"/content/ml_code_dataset.csv\"\n",
        "TARGET = \"energy_consumption_joules\"\n",
        "\n",
        "LEAKAGE_FEATURES = [\n",
        "    'co2_emissions_g', 'energy_per_line', 'time_per_line',\n",
        "    'co2_per_line', 'energy_pkg_joules', 'energy_ram_joules',\n",
        "    'power_draw_watts'\n",
        "]\n",
        "\n",
        "DYNAMIC_FEATURES = [\n",
        "    'execution_time', 'cpu_percent_avg', 'cpu_percent_max',\n",
        "    'memory_usage_mb', 'memory_peak_mb'\n",
        "]\n",
        "\n",
        "GRAY_FEATURES = [\n",
        "    'executed_successfully', 'execution_error',\n",
        "    'exit_code', 'syntax_valid', 'parse_error'\n",
        "]\n",
        "\n",
        "STATIC_FEATURES = [\n",
        "    'total_lines', 'code_lines', 'blank_lines', 'total_characters',\n",
        "    'file_size_bytes', 'avg_line_length', 'max_line_length',\n",
        "    'min_line_length', 'std_line_length', 'comment_lines',\n",
        "    'single_quotes', 'double_quotes', 'triple_quotes', 'total_quotes',\n",
        "    'parentheses_count', 'brackets_count', 'braces_count',\n",
        "    'semicolon_count', 'colon_count', 'unique_indentation_levels',\n",
        "    'avg_indentation', 'max_indentation', 'std_indentation',\n",
        "    'keyword_def_count', 'keyword_class_count', 'keyword_import_count',\n",
        "    'keyword_if_count', 'keyword_for_count', 'keyword_while_count',\n",
        "    'keyword_try_count', 'functions_count', 'classes_count',\n",
        "    'imports_count', 'if_statements', 'for_loops', 'while_loops',\n",
        "    'try_blocks', 'with_statements', 'lambda_functions',\n",
        "    'list_comprehensions', 'dict_comprehensions',\n",
        "    'generator_expressions', 'binary_operations',\n",
        "    'boolean_operations', 'comparisons', 'decorators', 'assignments',\n",
        "    'return_statements', 'cyclomatic_complexity', 'max_nesting_depth',\n",
        "    'total_ast_nodes', 'comment_ratio', 'control_flow_ratio',\n",
        "    'function_ratio', 'class_ratio', 'code_density',\n",
        "    'complexity_per_line'\n",
        "]\n",
        "\n",
        "METADATA_COLS = [\n",
        "    'file_path', 'file_name', 'repo_name', 'repo_url',\n",
        "    'timestamp', 'run_id', 'index', 'id', 'file_id',\n",
        "    'repository', 'author', 'date_created', 'date_modified',\n",
        "    'file_index'\n",
        "]\n",
        "\n",
        "# ===============================================================\n",
        "# STEP 1 â€” LOAD\n",
        "# ===============================================================\n",
        "\n",
        "print(\"=\"*90)\n",
        "print(\"FINAL HYBRID MODEL â€” STATIC + PSEUDO-DYNAMIC PROXIES + RANDOM FOREST\")\n",
        "print(\"=\"*90)\n",
        "\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "print(f\"Loaded dataset: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
        "\n",
        "if \"executed_successfully\" in df:\n",
        "    df = df[df[\"executed_successfully\"] == True]\n",
        "\n",
        "df = df[df[TARGET] > 0].dropna(subset=[TARGET])\n",
        "print(f\"Valid samples after filtering: {df.shape[0]}\")\n",
        "\n",
        "# ===============================================================\n",
        "# STEP 2 â€” SELECT FEATURES\n",
        "# ===============================================================\n",
        "\n",
        "usable_features = []\n",
        "for col in df.columns:\n",
        "    if col in STATIC_FEATURES or col in GRAY_FEATURES:\n",
        "        if col not in LEAKAGE_FEATURES + METADATA_COLS + [TARGET]:\n",
        "            usable_features.append(col)\n",
        "\n",
        "X = df[usable_features].copy()\n",
        "y = df[TARGET].copy()\n",
        "\n",
        "print(f\"Using {len(usable_features)} static + gray features\")\n",
        "\n",
        "# ===============================================================\n",
        "# STEP 3 â€” CLEANING\n",
        "# ===============================================================\n",
        "\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "for col in X.columns:\n",
        "    if X[col].isna().sum() > 0:\n",
        "        X[col].fillna(X[col].median(), inplace=True)\n",
        "\n",
        "constant_cols = [c for c in X.columns if X[c].nunique() <= 1]\n",
        "if constant_cols:\n",
        "    X.drop(columns=constant_cols, inplace=True)\n",
        "\n",
        "q_low, q_high = y.quantile(0.005), y.quantile(0.995)\n",
        "mask = (y >= q_low) & (y <= q_high)\n",
        "X, y = X.loc[mask], y.loc[mask]\n",
        "\n",
        "print(f\"Final cleaned dataset: {X.shape[0]} samples, {X.shape[1]} features\")\n",
        "\n",
        "# ===============================================================\n",
        "# STEP 4 â€” PROXY FEATURES\n",
        "# ===============================================================\n",
        "\n",
        "X_eng = X.copy()\n",
        "\n",
        "# Execution time proxy\n",
        "tmps = []\n",
        "if 'total_lines' in X: tmps.append(0.6 * X['total_lines'])\n",
        "if 'total_ast_nodes' in X: tmps.append(0.3 * X['total_ast_nodes'])\n",
        "if 'cyclomatic_complexity' in X: tmps.append(0.4 * X['cyclomatic_complexity'])\n",
        "\n",
        "if tmps:\n",
        "    t = sum(tmps)\n",
        "    t = (t - t.min()) / (t.max() - t.min() + 1e-9)\n",
        "    X_eng['est_execution_time_proxy'] = t * (X['total_lines'].median() + 1)\n",
        "\n",
        "# Memory proxy\n",
        "mc = []\n",
        "if 'total_ast_nodes' in X: mc.append(0.6 * X['total_ast_nodes'])\n",
        "if 'total_characters' in X: mc.append(0.2 * X['total_characters'])\n",
        "if mc:\n",
        "    m = sum(mc)\n",
        "    m = (m - m.min()) / (m.max() - m.min() + 1e-9)\n",
        "    X_eng['est_memory_proxy'] = m * (X['total_ast_nodes'].median() + 1)\n",
        "\n",
        "# CPU proxy\n",
        "cpu = []\n",
        "if 'for_loops' in X: cpu.append(0.5 * X['for_loops'])\n",
        "if 'max_nesting_depth' in X: cpu.append(0.4 * X['max_nesting_depth'])\n",
        "\n",
        "if cpu:\n",
        "    c = sum(cpu)\n",
        "    c = (c - c.min()) / (c.max() - c.min() + 1e-9)\n",
        "    X_eng['est_cpu_proxy'] = c * 100\n",
        "\n",
        "# Proxy interactions\n",
        "if 'est_execution_time_proxy' in X_eng:\n",
        "    X_eng['proxy_time_square'] = X_eng['est_execution_time_proxy'] ** 2\n",
        "\n",
        "if 'est_memory_proxy' in X_eng:\n",
        "    X_eng['proxy_mem_log'] = np.log1p(X_eng['est_memory_proxy'])\n",
        "\n",
        "if 'est_cpu_proxy' in X_eng and 'total_lines' in X_eng:\n",
        "    X_eng['proxy_cpu_times_lines'] = X_eng['est_cpu_proxy'] * X_eng['total_lines']\n",
        "\n",
        "# Polynomial features\n",
        "for col in ['total_lines', 'code_lines', 'cyclomatic_complexity', 'total_ast_nodes']:\n",
        "    if col in X_eng:\n",
        "        X_eng[f\"{col}_sqrt\"] = np.sqrt(X_eng[col] + 1)\n",
        "        X_eng[f\"{col}_log\"] = np.log1p(X_eng[col])\n",
        "        X_eng[f\"{col}_square\"] = X_eng[col] ** 2\n",
        "\n",
        "print(f\"Engineered features: {X_eng.shape[1]}\")\n",
        "\n",
        "# ===============================================================\n",
        "# STEP 5 â€” TARGET LOG\n",
        "# ===============================================================\n",
        "\n",
        "y_log = np.log1p(y)\n",
        "\n",
        "# ===============================================================\n",
        "# STEP 6 â€” SELECT BEST FEATURES\n",
        "# ===============================================================\n",
        "\n",
        "k = min(80, X_eng.shape[1])\n",
        "selector = SelectKBest(mutual_info_regression, k=k)\n",
        "selector.fit(X_eng.fillna(0), y_log)\n",
        "\n",
        "X_sel = X_eng.loc[:, selector.get_support()].fillna(0)\n",
        "\n",
        "print(f\"Selected top {X_sel.shape[1]} features\")\n",
        "\n",
        "# ===============================================================\n",
        "# STEP 7 â€” NORMALIZE\n",
        "# ===============================================================\n",
        "\n",
        "qt = QuantileTransformer(output_distribution=\"normal\", random_state=42)\n",
        "X_norm = pd.DataFrame(qt.fit_transform(X_sel), columns=X_sel.columns)\n",
        "\n",
        "# ===============================================================\n",
        "# STEP 8 â€” TRAIN/TEST SPLIT\n",
        "# ===============================================================\n",
        "\n",
        "bins = pd.qcut(y_log, q=5, labels=False, duplicates=\"drop\")\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_norm, y_log, test_size=0.15, random_state=42, stratify=bins\n",
        ")\n",
        "\n",
        "# ===============================================================\n",
        "# STEP 9 â€” STACKING ENSEMBLE (WITH RANDOM FOREST)\n",
        "# ===============================================================\n",
        "\n",
        "print(\"\\nTraining Stacking Ensemble with RF...\")\n",
        "\n",
        "base_models = [\n",
        "    (\"lgbm\", LGBMRegressor(\n",
        "        n_estimators=1000, learning_rate=0.02,\n",
        "        num_leaves=50, random_state=42, n_jobs=-1, verbose=-1\n",
        "    )),\n",
        "    (\"xgb\", XGBRegressor(\n",
        "        n_estimators=800, learning_rate=0.02,\n",
        "        max_depth=8, n_jobs=-1, verbosity=0, random_state=42\n",
        "    )),\n",
        "    (\"rf\", RandomForestRegressor(\n",
        "        n_estimators=600, n_jobs=-1, random_state=42\n",
        "    )),\n",
        "    (\"ridge\", Ridge(alpha=1.0)),\n",
        "    (\"elastic\", ElasticNet(alpha=0.001, l1_ratio=0.5, max_iter=5000))\n",
        "]\n",
        "\n",
        "meta_model = Ridge(alpha=1.0)\n",
        "\n",
        "model = StackingRegressor(\n",
        "    estimators=base_models,\n",
        "    final_estimator=meta_model,\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# ===============================================================\n",
        "# STEP 10 â€” EVALUATE\n",
        "# ===============================================================\n",
        "\n",
        "y_pred_test_log = model.predict(X_test)\n",
        "y_pred_test = np.expm1(y_pred_test_log)\n",
        "y_actual_test = np.expm1(y_test)\n",
        "\n",
        "r2_test = r2_score(y_test, y_pred_test_log)\n",
        "mae = mean_absolute_error(y_actual_test, y_pred_test)\n",
        "rmse = np.sqrt(mean_squared_error(y_actual_test, y_pred_test))\n",
        "\n",
        "print(\"\\nRESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Test RÂ² (log): {r2_test:.4f}\")\n",
        "print(f\"MAE:           {mae:.2f} J\")\n",
        "print(f\"RMSE:          {rmse:.2f} J\")\n",
        "\n",
        "# ===============================================================\n",
        "# STEP 11 â€” CROSS VAL\n",
        "# ===============================================================\n",
        "\n",
        "cv_scores = cross_val_score(model, X_norm, y_log, cv=5, scoring=\"r2\", n_jobs=-1)\n",
        "print(\"\\nCross-validation RÂ²:\", cv_scores)\n",
        "print(\"Mean CV:\", cv_scores.mean())\n",
        "\n",
        "print(\"\\nðŸŽ‰ DONE â€” FINAL HYBRID MODEL WITH RF SUCCESSFULLY RUN.\")\n"
      ],
      "metadata": {
        "id": "Dh8MgrTjmp5Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f1cb655a-3b52-446c-ab96-52efa09c0c31"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========================================================================================\n",
            "FINAL HYBRID MODEL â€” STATIC + PSEUDO-DYNAMIC PROXIES + RANDOM FOREST\n",
            "==========================================================================================\n",
            "Loaded dataset: 6711 rows, 107 columns\n",
            "Valid samples after filtering: 1567\n",
            "Using 62 static + gray features\n",
            "Final cleaned dataset: 1551 samples, 58 features\n",
            "Engineered features: 76\n",
            "Selected top 76 features\n",
            "\n",
            "Training Stacking Ensemble with RF...\n",
            "\n",
            "RESULTS\n",
            "======================================================================\n",
            "Test RÂ² (log): 0.5477\n",
            "MAE:           32.91 J\n",
            "RMSE:          67.85 J\n",
            "\n",
            "Cross-validation RÂ²: [ 0.59746617  0.49754899 -0.24223204  0.57130904  0.57001247]\n",
            "Mean CV: 0.3988209266781783\n",
            "\n",
            "ðŸŽ‰ DONE â€” FINAL HYBRID MODEL WITH RF SUCCESSFULLY RUN.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import StackingRegressor\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\n",
        "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================================\n",
        "DATASET_PATH = '/content/ml_code_dataset.csv'\n",
        "\n",
        "DYNAMIC_FEATURES = [\n",
        "    'execution_time', 'cpu_percent_avg', 'cpu_percent_max',\n",
        "    'memory_usage_mb', 'memory_peak_mb', 'co2_emissions_g',\n",
        "    'energy_per_line', 'time_per_line', 'co2_per_line'\n",
        "]\n",
        "\n",
        "GRAY_FEATURES = [\n",
        "    'executed_successfully', 'execution_error', 'exit_code',\n",
        "    'syntax_valid', 'parse_error', 'syntax_error_line', 'syntax_error_type'\n",
        "]\n",
        "\n",
        "STATIC_FEATURES = [\n",
        "    'total_lines', 'code_lines', 'blank_lines', 'total_characters',\n",
        "    'file_size_bytes', 'file_size_kb', 'avg_line_length', 'max_line_length',\n",
        "    'min_line_length', 'std_line_length', 'comment_lines', 'single_quotes',\n",
        "    'double_quotes', 'triple_quotes', 'total_quotes', 'parentheses_count',\n",
        "    'brackets_count', 'braces_count', 'semicolon_count', 'colon_count',\n",
        "    'unique_indentation_levels', 'avg_indentation', 'max_indentation',\n",
        "    'std_indentation', 'keyword_def_count', 'keyword_class_count',\n",
        "    'keyword_import_count', 'keyword_from_count', 'keyword_if_count',\n",
        "    'keyword_else_count', 'keyword_elif_count', 'keyword_for_count',\n",
        "    'keyword_while_count', 'keyword_try_count', 'keyword_except_count',\n",
        "    'keyword_finally_count', 'keyword_with_count', 'keyword_return_count',\n",
        "    'keyword_yield_count', 'keyword_lambda_count', 'keyword_async_count',\n",
        "    'keyword_await_count', 'keyword_global_count', 'keyword_nonlocal_count',\n",
        "    'functions_count', 'async_functions_count', 'classes_count',\n",
        "    'imports_count', 'if_statements', 'for_loops', 'while_loops',\n",
        "    'async_for_loops', 'try_blocks', 'with_statements', 'async_with_statements',\n",
        "    'lambda_functions', 'list_comprehensions', 'dict_comprehensions',\n",
        "    'set_comprehensions', 'generator_expressions', 'binary_operations',\n",
        "    'unary_operations', 'boolean_operations', 'comparisons', 'decorators',\n",
        "    'assignments', 'augmented_assignments', 'return_statements',\n",
        "    'yield_statements', 'raise_statements', 'assert_statements',\n",
        "    'cyclomatic_complexity', 'max_nesting_depth', 'unique_function_names',\n",
        "    'unique_class_names', 'unique_imports', 'unique_variables',\n",
        "    'total_ast_nodes', 'comment_ratio', 'control_flow_ratio',\n",
        "    'function_ratio', 'class_ratio', 'code_density', 'complexity_per_line',\n",
        "    'functions_per_line', 'avg_complexity_per_function'\n",
        "]\n",
        "\n",
        "METADATA_COLS = [\n",
        "    'file_path', 'file_name', 'repo_name', 'repo_url', 'timestamp',\n",
        "    'run_id', 'index', 'id', 'file_id', 'repository', 'author',\n",
        "    'date_created', 'date_modified', 'file_index'\n",
        "]\n",
        "\n",
        "TARGET = 'energy_consumption_joules'\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 1: LOAD DATA\n",
        "# ============================================================================\n",
        "print(\"=\" * 80)\n",
        "print(\"PSEUDO-DYNAMIC FEATURES MODEL - BRIDGE THE GAP\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "df = pd.read_csv(DATASET_PATH)\n",
        "print(f\"\\nðŸ“Š Initial Dataset: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
        "\n",
        "if 'executed_successfully' in df.columns:\n",
        "    df = df[df['executed_successfully'] == True].copy()\n",
        "\n",
        "df = df[df[TARGET] > 0].copy()\n",
        "df = df.dropna(subset=[TARGET])\n",
        "print(f\"âœ… Valid samples: {df.shape[0]:,} rows\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 2: BUILD BASE FEATURES\n",
        "# ============================================================================\n",
        "available_cols = df.columns.tolist()\n",
        "usable_features = [col for col in available_cols\n",
        "                   if (col in STATIC_FEATURES or col in GRAY_FEATURES)\n",
        "                   and col not in [TARGET] + METADATA_COLS + DYNAMIC_FEATURES]\n",
        "\n",
        "X = df[usable_features].copy()\n",
        "y = df[TARGET].copy()\n",
        "\n",
        "# Clean\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "for col in X.columns:\n",
        "    if X[col].dtype in ['float64', 'int64']:\n",
        "        if X[col].isna().sum() > 0:\n",
        "            X[col].fillna(X[col].median(), inplace=True)\n",
        "\n",
        "constant_cols = [col for col in X.columns if X[col].nunique() <= 1]\n",
        "if constant_cols:\n",
        "    X = X.drop(columns=constant_cols)\n",
        "\n",
        "# Minimal outlier removal\n",
        "q_low = y.quantile(0.005)\n",
        "q_high = y.quantile(0.995)\n",
        "mask = (y >= q_low) & (y <= q_high)\n",
        "X = X[mask]\n",
        "y = y[mask]\n",
        "\n",
        "print(f\"ðŸ“ Base: {X.shape[0]:,} samples, {X.shape[1]} features\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 3: CREATE PSEUDO-DYNAMIC FEATURES\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸŽ¯ Creating Pseudo-Dynamic Features (Static â†’ Runtime Proxies)...\")\n",
        "\n",
        "X_pseudo = X.copy()\n",
        "\n",
        "# === ESTIMATED EXECUTION TIME PROXIES ===\n",
        "print(\"   â€¢ Execution time proxies...\")\n",
        "\n",
        "# More loops = longer execution\n",
        "if 'for_loops' in X_pseudo.columns and 'while_loops' in X_pseudo.columns:\n",
        "    X_pseudo['estimated_loop_cost'] = (\n",
        "        X_pseudo['for_loops'] * 10 +\n",
        "        X_pseudo['while_loops'] * 15 +  # while loops often slower\n",
        "        X_pseudo.get('async_for_loops', 0) * 12\n",
        "    )\n",
        "\n",
        "# Nested complexity = exponential time\n",
        "if 'max_nesting_depth' in X_pseudo.columns and 'cyclomatic_complexity' in X_pseudo.columns:\n",
        "    X_pseudo['estimated_time_complexity'] = (\n",
        "        X_pseudo['max_nesting_depth'] ** 2 *\n",
        "        X_pseudo['cyclomatic_complexity']\n",
        "    )\n",
        "\n",
        "# Lines Ã— complexity = rough execution proxy\n",
        "if 'code_lines' in X_pseudo.columns and 'cyclomatic_complexity' in X_pseudo.columns:\n",
        "    X_pseudo['estimated_ops_count'] = (\n",
        "        X_pseudo['code_lines'] *\n",
        "        (X_pseudo['cyclomatic_complexity'] + 1) ** 1.5\n",
        "    )\n",
        "\n",
        "# Function calls (more functions = more overhead)\n",
        "if 'functions_count' in X_pseudo.columns and 'total_lines' in X_pseudo.columns:\n",
        "    X_pseudo['estimated_call_overhead'] = (\n",
        "        X_pseudo['functions_count'] *\n",
        "        np.log1p(X_pseudo['total_lines'])\n",
        "    )\n",
        "\n",
        "# === ESTIMATED MEMORY PROXIES ===\n",
        "print(\"   â€¢ Memory usage proxies...\")\n",
        "\n",
        "# More variables = more memory\n",
        "if 'unique_variables' in X_pseudo.columns:\n",
        "    X_pseudo['estimated_var_memory'] = X_pseudo['unique_variables'] * np.log1p(X_pseudo['unique_variables'])\n",
        "\n",
        "# Data structures = memory hogs\n",
        "if 'list_comprehensions' in X_pseudo.columns:\n",
        "    X_pseudo['estimated_list_memory'] = X_pseudo['list_comprehensions'] * 20\n",
        "if 'dict_comprehensions' in X_pseudo.columns:\n",
        "    X_pseudo['estimated_dict_memory'] = X_pseudo['dict_comprehensions'] * 30\n",
        "if 'set_comprehensions' in X_pseudo.columns:\n",
        "    X_pseudo['estimated_set_memory'] = X_pseudo['set_comprehensions'] * 25\n",
        "\n",
        "# Total estimated data structure memory\n",
        "X_pseudo['estimated_datastructure_cost'] = (\n",
        "    X_pseudo.get('estimated_list_memory', 0) +\n",
        "    X_pseudo.get('estimated_dict_memory', 0) +\n",
        "    X_pseudo.get('estimated_set_memory', 0)\n",
        ")\n",
        "\n",
        "# Classes = objects = memory\n",
        "if 'classes_count' in X_pseudo.columns and 'unique_variables' in X_pseudo.columns:\n",
        "    X_pseudo['estimated_object_memory'] = (\n",
        "        X_pseudo['classes_count'] *\n",
        "        X_pseudo['unique_variables'] * 2\n",
        "    )\n",
        "\n",
        "# === ESTIMATED CPU INTENSITY ===\n",
        "print(\"   â€¢ CPU intensity proxies...\")\n",
        "\n",
        "# Nested loops = CPU intensive\n",
        "if 'for_loops' in X_pseudo.columns and 'max_nesting_depth' in X_pseudo.columns:\n",
        "    X_pseudo['estimated_cpu_intensity'] = (\n",
        "        X_pseudo['for_loops'] *\n",
        "        (X_pseudo['max_nesting_depth'] ** 2)\n",
        "    )\n",
        "\n",
        "# Binary operations = computation\n",
        "if 'binary_operations' in X_pseudo.columns:\n",
        "    X_pseudo['estimated_computation_load'] = (\n",
        "        X_pseudo['binary_operations'] *\n",
        "        np.log1p(X_pseudo['binary_operations'])\n",
        "    )\n",
        "\n",
        "# Comparisons in loops = intensive\n",
        "if 'comparisons' in X_pseudo.columns and 'for_loops' in X_pseudo.columns:\n",
        "    X_pseudo['estimated_comparison_cost'] = (\n",
        "        X_pseudo['comparisons'] *\n",
        "        (X_pseudo['for_loops'] + 1)\n",
        "    )\n",
        "\n",
        "# === LIBRARY/IO PROXIES ===\n",
        "print(\"   â€¢ I/O and library proxies...\")\n",
        "\n",
        "# More imports = potential for expensive operations\n",
        "if 'imports_count' in X_pseudo.columns:\n",
        "    X_pseudo['estimated_library_overhead'] = (\n",
        "        X_pseudo['imports_count'] ** 1.5\n",
        "    )\n",
        "\n",
        "# File operations proxy (try/except suggests I/O)\n",
        "if 'try_blocks' in X_pseudo.columns and 'with_statements' in X_pseudo.columns:\n",
        "    X_pseudo['estimated_io_operations'] = (\n",
        "        X_pseudo['try_blocks'] * 5 +\n",
        "        X_pseudo['with_statements'] * 10\n",
        "    )\n",
        "\n",
        "# Async operations = I/O bound\n",
        "if 'async_functions_count' in X_pseudo.columns:\n",
        "    X_pseudo['estimated_async_cost'] = (\n",
        "        X_pseudo['async_functions_count'] * 15\n",
        "    )\n",
        "if 'keyword_await_count' in X_pseudo.columns:\n",
        "    X_pseudo['estimated_await_cost'] = (\n",
        "        X_pseudo['keyword_await_count'] * 8\n",
        "    )\n",
        "\n",
        "# === ALGORITHMIC COMPLEXITY PROXIES ===\n",
        "print(\"   â€¢ Algorithmic complexity proxies...\")\n",
        "\n",
        "# Nested loops = O(nÂ²) or worse\n",
        "if 'for_loops' in X_pseudo.columns and 'while_loops' in X_pseudo.columns:\n",
        "    loops_total = X_pseudo['for_loops'] + X_pseudo['while_loops']\n",
        "    X_pseudo['estimated_algo_complexity_quadratic'] = loops_total ** 2\n",
        "    X_pseudo['estimated_algo_complexity_cubic'] = loops_total ** 3\n",
        "\n",
        "# Recursion proxy (functions calling functions)\n",
        "if 'functions_count' in X_pseudo.columns and 'keyword_return_count' in X_pseudo.columns:\n",
        "    X_pseudo['estimated_recursion_depth'] = (\n",
        "        X_pseudo['functions_count'] *\n",
        "        X_pseudo['keyword_return_count'] / (X_pseudo['total_lines'] + 1)\n",
        "    )\n",
        "\n",
        "# List comprehensions in loops = nested iteration\n",
        "if 'list_comprehensions' in X_pseudo.columns and 'for_loops' in X_pseudo.columns:\n",
        "    X_pseudo['estimated_nested_iteration'] = (\n",
        "        X_pseudo['list_comprehensions'] *\n",
        "        X_pseudo['for_loops']\n",
        "    )\n",
        "\n",
        "# === CODE COMPLEXITY INDICATORS ===\n",
        "print(\"   â€¢ Code complexity indicators...\")\n",
        "\n",
        "# Deep nesting = hard to optimize\n",
        "if 'max_nesting_depth' in X_pseudo.columns:\n",
        "    X_pseudo['nesting_penalty'] = X_pseudo['max_nesting_depth'] ** 3\n",
        "\n",
        "# High cyclomatic complexity = many branches\n",
        "if 'cyclomatic_complexity' in X_pseudo.columns:\n",
        "    X_pseudo['branch_penalty'] = X_pseudo['cyclomatic_complexity'] ** 2\n",
        "\n",
        "# Code density = packed logic\n",
        "if 'code_density' in X_pseudo.columns:\n",
        "    X_pseudo['density_penalty'] = X_pseudo['code_density'] ** 2\n",
        "\n",
        "# === COMBINED ENERGY SCORE ===\n",
        "print(\"   â€¢ Combined energy score...\")\n",
        "\n",
        "# Weighted combination of all proxies\n",
        "X_pseudo['estimated_energy_score'] = (\n",
        "    X_pseudo.get('estimated_ops_count', 0) * 0.3 +\n",
        "    X_pseudo.get('estimated_loop_cost', 0) * 0.2 +\n",
        "    X_pseudo.get('estimated_cpu_intensity', 0) * 0.15 +\n",
        "    X_pseudo.get('estimated_var_memory', 0) * 0.1 +\n",
        "    X_pseudo.get('estimated_io_operations', 0) * 0.1 +\n",
        "    X_pseudo.get('estimated_library_overhead', 0) * 0.05 +\n",
        "    X_pseudo.get('nesting_penalty', 0) * 0.05 +\n",
        "    X_pseudo.get('branch_penalty', 0) * 0.05\n",
        ")\n",
        "\n",
        "print(f\"\\nâœ… Pseudo-dynamic features created: {X_pseudo.shape[1]} total features\")\n",
        "print(f\"   Original: {X.shape[1]}\")\n",
        "print(f\"   New pseudo-dynamic: {X_pseudo.shape[1] - X.shape[1]}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 4: STANDARD FEATURE ENGINEERING\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸ”§ Standard Feature Engineering...\")\n",
        "\n",
        "# Power transforms\n",
        "for col in ['total_lines', 'code_lines', 'cyclomatic_complexity',\n",
        "            'total_ast_nodes', 'estimated_energy_score']:\n",
        "    if col in X_pseudo.columns:\n",
        "        X_pseudo[f'{col}_sqrt'] = np.sqrt(X_pseudo[col] + 1)\n",
        "        X_pseudo[f'{col}_log'] = np.log1p(X_pseudo[col])\n",
        "        X_pseudo[f'{col}_square'] = X_pseudo[col] ** 2\n",
        "\n",
        "# Key interactions\n",
        "if 'estimated_energy_score' in X_pseudo.columns:\n",
        "    if 'total_lines' in X_pseudo.columns:\n",
        "        X_pseudo['energy_per_line_proxy'] = X_pseudo['estimated_energy_score'] / (X_pseudo['total_lines'] + 1)\n",
        "    if 'cyclomatic_complexity' in X_pseudo.columns:\n",
        "        X_pseudo['energy_x_complexity'] = X_pseudo['estimated_energy_score'] * X_pseudo['cyclomatic_complexity']\n",
        "\n",
        "print(f\"âœ… Total features: {X_pseudo.shape[1]}\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 5: LOG TRANSFORM TARGET\n",
        "# ============================================================================\n",
        "y_log = np.log1p(y)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 6: FEATURE SELECTION\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸŽ¯ Feature Selection...\")\n",
        "\n",
        "k_best = min(70, X_pseudo.shape[1])\n",
        "selector = SelectKBest(score_func=mutual_info_regression, k=k_best)\n",
        "selector.fit(X_pseudo, y_log)\n",
        "X_selected = X_pseudo.loc[:, selector.get_support()]\n",
        "\n",
        "print(f\"âœ… Selected {X_selected.shape[1]} best features\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 7: NORMALIZE\n",
        "# ============================================================================\n",
        "qt = QuantileTransformer(n_quantiles=min(1000, X_selected.shape[0]),\n",
        "                         output_distribution='normal', random_state=42)\n",
        "X_transformed = pd.DataFrame(\n",
        "    qt.fit_transform(X_selected),\n",
        "    columns=X_selected.columns,\n",
        "    index=X_selected.index\n",
        ")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 8: STRATIFIED SPLIT\n",
        "# ============================================================================\n",
        "y_bins = pd.qcut(y_log, q=5, labels=False, duplicates='drop')\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_transformed, y_log, test_size=0.15, random_state=42,\n",
        "    stratify=y_bins, shuffle=True\n",
        ")\n",
        "\n",
        "print(f\"\\nðŸ“Š Split: {X_train.shape[0]:,} train, {X_test.shape[0]:,} test\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 9: STACKING ENSEMBLE\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸš€ Training Stacking Ensemble with Pseudo-Dynamic Features...\")\n",
        "\n",
        "base_models = [\n",
        "    ('lgbm1', LGBMRegressor(\n",
        "        n_estimators=1200, learning_rate=0.008, num_leaves=40,\n",
        "        max_depth=8, min_child_samples=15, subsample=0.85,\n",
        "        colsample_bytree=0.85, reg_alpha=0.3, reg_lambda=0.3,\n",
        "        random_state=42, verbose=-1, n_jobs=-1\n",
        "    )),\n",
        "    ('lgbm2', LGBMRegressor(\n",
        "        n_estimators=1000, learning_rate=0.012, num_leaves=25,\n",
        "        max_depth=6, min_child_samples=25, subsample=0.75,\n",
        "        colsample_bytree=0.75, reg_alpha=0.8, reg_lambda=0.8,\n",
        "        random_state=43, verbose=-1, n_jobs=-1\n",
        "    )),\n",
        "    ('xgb1', XGBRegressor(\n",
        "        n_estimators=1200, learning_rate=0.008, max_depth=8,\n",
        "        min_child_weight=2, subsample=0.85, colsample_bytree=0.85,\n",
        "        reg_alpha=0.3, reg_lambda=0.3, random_state=42,\n",
        "        verbosity=0, n_jobs=-1\n",
        "    )),\n",
        "    ('xgb2', XGBRegressor(\n",
        "        n_estimators=1000, learning_rate=0.012, max_depth=6,\n",
        "        min_child_weight=4, subsample=0.75, colsample_bytree=0.75,\n",
        "        reg_alpha=0.8, reg_lambda=0.8, random_state=43,\n",
        "        verbosity=0, n_jobs=-1\n",
        "    )),\n",
        "    ('ridge', Ridge(alpha=5.0)),\n",
        "    ('elastic', ElasticNet(alpha=0.005, l1_ratio=0.5, max_iter=5000))\n",
        "]\n",
        "\n",
        "meta_model = Ridge(alpha=3.0)\n",
        "\n",
        "stacking_model = StackingRegressor(\n",
        "    estimators=base_models,\n",
        "    final_estimator=meta_model,\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "stacking_model.fit(X_train, y_train)\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 10: EVALUATE\n",
        "# ============================================================================\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(\"MODEL PERFORMANCE WITH PSEUDO-DYNAMIC FEATURES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "y_train_pred_log = stacking_model.predict(X_train)\n",
        "y_test_pred_log = stacking_model.predict(X_test)\n",
        "\n",
        "r2_train_log = r2_score(y_train, y_train_pred_log)\n",
        "r2_test_log = r2_score(y_test, y_test_pred_log)\n",
        "\n",
        "print(f\"\\nðŸ“Š Log-Scale Performance:\")\n",
        "print(f\"   Training RÂ²: {r2_train_log:.6f}\")\n",
        "print(f\"   Test RÂ²:     {r2_test_log:.6f}\")\n",
        "print(f\"   Gap: {r2_train_log - r2_test_log:.6f}\")\n",
        "\n",
        "y_train_actual = np.expm1(y_train)\n",
        "y_train_pred = np.expm1(y_train_pred_log)\n",
        "y_test_actual = np.expm1(y_test)\n",
        "y_test_pred = np.expm1(y_test_pred_log)\n",
        "\n",
        "r2_test_original = r2_score(y_test_actual, y_test_pred)\n",
        "mae_test = mean_absolute_error(y_test_actual, y_test_pred)\n",
        "rmse_test = np.sqrt(mean_squared_error(y_test_actual, y_test_pred))\n",
        "mean_energy = y_test_actual.mean()\n",
        "\n",
        "print(f\"\\nâš¡ Original Joules Scale:\")\n",
        "print(f\"   Test RÂ²: {r2_test_original:.6f}\")\n",
        "print(f\"   MAE: {mae_test:,.2f} J ({(mae_test/mean_energy)*100:.1f}%)\")\n",
        "print(f\"   RMSE: {rmse_test:,.2f} J\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 11: CROSS-VALIDATION\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸ”„ 5-Fold Cross-Validation:\")\n",
        "cv_scores = cross_val_score(\n",
        "    stacking_model, X_transformed, y_log, cv=5,\n",
        "    scoring='r2', n_jobs=-1\n",
        ")\n",
        "print(f\"   CV RÂ²: {[f'{s:.4f}' for s in cv_scores]}\")\n",
        "print(f\"   Mean: {cv_scores.mean():.6f} (Â±{cv_scores.std():.6f})\")\n",
        "\n",
        "# ============================================================================\n",
        "# STEP 12: FEATURE IMPORTANCE (TOP PSEUDO-DYNAMIC)\n",
        "# ============================================================================\n",
        "print(f\"\\nðŸŽ¯ Top 15 Features (including pseudo-dynamic):\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# Get feature importance from LGBM base model\n",
        "lgbm_model = base_models[0][1]\n",
        "lgbm_model.fit(X_train, y_train)\n",
        "\n",
        "feature_importance = pd.DataFrame({\n",
        "    'feature': X_train.columns,\n",
        "    'importance': lgbm_model.feature_importances_\n",
        "}).sort_values('importance', ascending=False)\n",
        "\n",
        "for idx, row in feature_importance.head(15).iterrows():\n",
        "    marker = \"ðŸ”¥\" if 'estimated' in row['feature'] or 'penalty' in row['feature'] else \"  \"\n",
        "    print(f\"   {marker} {row['feature']:45s} {row['importance']:8.1f}\")\n",
        "\n",
        "# ============================================================================\n",
        "# FINAL SUMMARY\n",
        "# ============================================================================\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "print(\"ðŸŽ‰ FINAL RESULTS - WITH PSEUDO-DYNAMIC FEATURES\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"âœ… Samples: {X_train.shape[0]:,} train, {X_test.shape[0]:,} test\")\n",
        "print(f\"âœ… Features: {X_transformed.shape[1]} (with pseudo-dynamic proxies)\")\n",
        "print(f\"âœ… Test RÂ² (Log): {r2_test_log:.6f}\")\n",
        "print(f\"âœ… Test RÂ² (Original): {r2_test_original:.6f}\")\n",
        "print(f\"âœ… CV RÂ²: {cv_scores.mean():.6f} Â±{cv_scores.std():.4f}\")\n",
        "print(f\"âœ… MAE: {mae_test:,.2f} J ({(mae_test/mean_energy)*100:.1f}%)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if r2_test_log >= 0.8:\n",
        "    print(\"\\nðŸŽŠ BREAKTHROUGH: RÂ² â‰¥ 0.80! ðŸŽŠ\")\n",
        "elif r2_test_log >= 0.7:\n",
        "    print(f\"\\nðŸš€ Excellent: RÂ² = {r2_test_log:.3f} - Pseudo-dynamic features working!\")\n",
        "elif r2_test_log >= 0.6:\n",
        "    print(f\"\\nâœ… Strong: RÂ² = {r2_test_log:.3f} - Significant improvement!\")\n",
        "else:\n",
        "    print(f\"\\nðŸ“Š Current: RÂ² = {r2_test_log:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h0_S42XuKG_Q",
        "outputId": "83294023-3d1a-46fc-cc16-52f956f9d83a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "PSEUDO-DYNAMIC FEATURES MODEL - BRIDGE THE GAP\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š Initial Dataset: 6,711 rows Ã— 107 columns\n",
            "âœ… Valid samples: 1,567 rows\n",
            "ðŸ“ Base: 1,551 samples, 87 features\n",
            "\n",
            "ðŸŽ¯ Creating Pseudo-Dynamic Features (Static â†’ Runtime Proxies)...\n",
            "   â€¢ Execution time proxies...\n",
            "   â€¢ Memory usage proxies...\n",
            "   â€¢ CPU intensity proxies...\n",
            "   â€¢ I/O and library proxies...\n",
            "   â€¢ Algorithmic complexity proxies...\n",
            "   â€¢ Code complexity indicators...\n",
            "   â€¢ Combined energy score...\n",
            "\n",
            "âœ… Pseudo-dynamic features created: 112 total features\n",
            "   Original: 87\n",
            "   New pseudo-dynamic: 25\n",
            "\n",
            "ðŸ”§ Standard Feature Engineering...\n",
            "âœ… Total features: 129\n",
            "\n",
            "ðŸŽ¯ Feature Selection...\n",
            "âœ… Selected 70 best features\n",
            "\n",
            "ðŸ“Š Split: 1,318 train, 233 test\n",
            "\n",
            "ðŸš€ Training Stacking Ensemble with Pseudo-Dynamic Features...\n",
            "\n",
            "================================================================================\n",
            "MODEL PERFORMANCE WITH PSEUDO-DYNAMIC FEATURES\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š Log-Scale Performance:\n",
            "   Training RÂ²: 0.890703\n",
            "   Test RÂ²:     0.527900\n",
            "   Gap: 0.362803\n",
            "\n",
            "âš¡ Original Joules Scale:\n",
            "   Test RÂ²: 0.304367\n",
            "   MAE: 34.23 J (63.0%)\n",
            "   RMSE: 70.10 J\n",
            "\n",
            "ðŸ”„ 5-Fold Cross-Validation:\n",
            "   CV RÂ²: ['0.5969', '0.5035', '-0.2260', '0.6060', '0.5859']\n",
            "   Mean: 0.413258 (Â±0.321728)\n",
            "\n",
            "ðŸŽ¯ Top 15 Features (including pseudo-dynamic):\n",
            "----------------------------------------------------------------------\n",
            "      comment_ratio                                   1626.0\n",
            "      code_density                                    1472.0\n",
            "      control_flow_ratio                              1337.0\n",
            "      complexity_per_line                             1276.0\n",
            "      function_ratio                                  1182.0\n",
            "      avg_indentation                                 1147.0\n",
            "      parentheses_count                               1091.0\n",
            "      binary_operations                               1063.0\n",
            "      functions_per_line                              1046.0\n",
            "      total_quotes                                     995.0\n",
            "      energy_per_line_proxy                            965.0\n",
            "      double_quotes                                    956.0\n",
            "   ðŸ”¥ estimated_recursion_depth                        911.0\n",
            "      single_quotes                                    891.0\n",
            "      colon_count                                      839.0\n",
            "\n",
            "================================================================================\n",
            "ðŸŽ‰ FINAL RESULTS - WITH PSEUDO-DYNAMIC FEATURES\n",
            "================================================================================\n",
            "âœ… Samples: 1,318 train, 233 test\n",
            "âœ… Features: 70 (with pseudo-dynamic proxies)\n",
            "âœ… Test RÂ² (Log): 0.527900\n",
            "âœ… Test RÂ² (Original): 0.304367\n",
            "âœ… CV RÂ²: 0.413258 Â±0.3217\n",
            "âœ… MAE: 34.23 J (63.0%)\n",
            "================================================================================\n",
            "\n",
            "ðŸ“Š Current: RÂ² = 0.528\n"
          ]
        }
      ]
    }
  ]
}